{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_18232\\3060230758.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  owner_scm_df_selected.rename(columns={'Base Lead Time (Days)': 'LeadTime'}, inplace=True)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_18232\\3060230758.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].astype(str)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_18232\\3060230758.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].fillna('')\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_18232\\3060230758.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].apply(lambda x: x.split('.')[0] if '.0' in str(x) else x)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_18232\\3060230758.py:109: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  access_df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_18232\\3060230758.py:110: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  query2_df = pd.read_sql(query2, conn)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_18232\\3060230758.py:111: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  query3_df = pd.read_sql(query3, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to Access database is closed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_18232\\3060230758.py:256: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Current_DC4_DOH'] = calculate_DOH(merged_df['DC4_Remain_StockQty'], merged_df['DC4_AvgSaleQty90D'])\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_18232\\3060230758.py:259: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Total-PO_qty_to_DC'] = merged_df['Total-PO_qty_to_DC1'] + merged_df['Total-PO_qty_to_DC2'] + merged_df['Total-PO_qty_to_DC4']\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_18232\\3060230758.py:260: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Current_DOH(Stock+PO)_All_DC'] = calculate_DOH(merged_df['Remain_StockQty_AllDC'] + merged_df['Total-PO_qty_to_DC'], merged_df['Total_AvgSaleQty90D'])\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_18232\\3060230758.py:261: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['DC1_DOH(Stock+PO)'] = calculate_DOH(merged_df['DC1_Remain_StockQty'] + merged_df['Total-PO_qty_to_DC1'], merged_df['DC1_AvgSaleQty90D'])\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_18232\\3060230758.py:262: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['DC2_DOH(Stock+PO)'] = calculate_DOH(merged_df['DC2_Remain_StockQty'] + merged_df['Total-PO_qty_to_DC2'], merged_df['DC2_AvgSaleQty90D'])\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_18232\\3060230758.py:263: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['DC4_DOH(Stock+PO)'] = calculate_DOH(merged_df['DC4_Remain_StockQty'] + merged_df['Total-PO_qty_to_DC4'], merged_df['DC4_AvgSaleQty90D'])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'Timestamp' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 632\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while saving the file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 632\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 385\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    383\u001b[0m  merged_df \u001b[38;5;241m=\u001b[39m rearrange_columns(merged_df)\n\u001b[0;32m    384\u001b[0m  merged_df \u001b[38;5;241m=\u001b[39m calculate_totals(merged_df)\n\u001b[1;32m--> 385\u001b[0m  merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mapply_doh_calculations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;66;03m# Execute SQL query\u001b[39;00m\n\u001b[0;32m    388\u001b[0m  query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124m SELECT Division_SHM,\u001b[39m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124m     OwnerSCM,\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;124m WHERE ([Group] != \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiscontinuous\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or [Group] IS NULL)\u001b[39m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 266\u001b[0m, in \u001b[0;36mapply_doh_calculations\u001b[1;34m(merged_df)\u001b[0m\n\u001b[0;32m    262\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDC2_DOH(Stock+PO)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m calculate_DOH(merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDC2_Remain_StockQty\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal-PO_qty_to_DC2\u001b[39m\u001b[38;5;124m'\u001b[39m], merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDC2_AvgSaleQty90D\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    263\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDC4_DOH(Stock+PO)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m calculate_DOH(merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDC4_Remain_StockQty\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal-PO_qty_to_DC4\u001b[39m\u001b[38;5;124m'\u001b[39m], merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDC4_AvgSaleQty90D\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 266\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin_delivery_date_to_DC1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMin_del_date_to_DC1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMin_del_date_to_DC1_from-Import\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMin_del_date_to_DC1_from-HBA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin_delivery_date_to_DC2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m merged_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin_del_date_to_DC2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin_del_date_to_DC2_from-Import\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin_del_date_to_DC2_from-HBA\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    268\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin_delivery_date_to_DC4\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m merged_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin_del_date_to_DC4\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin_del_date_to_DC4_from-Import\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin_del_date_to_DC4_from-HBA\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Thanawit C\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:11643\u001b[0m, in \u001b[0;36mDataFrame.min\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11635\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m  11636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmin\u001b[39m(\n\u001b[0;32m  11637\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11641\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11642\u001b[0m ):\n\u001b[1;32m> 11643\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  11644\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[0;32m  11645\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Thanawit C\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:12388\u001b[0m, in \u001b[0;36mNDFrame.min\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  12381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmin\u001b[39m(\n\u001b[0;32m  12382\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  12383\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12386\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  12387\u001b[0m ):\n\u001b[1;32m> 12388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  12389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12391\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12394\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  12395\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thanawit C\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:12377\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  12373\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[0;32m  12375\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m> 12377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  12378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[0;32m  12379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thanawit C\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:11562\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m  11558\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m  11560\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[0;32m  11561\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[1;32m> 11562\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  11563\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(res, axes\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m  11564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboolean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Thanawit C\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1500\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m   1498\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m-> 1500\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[0;32m   1503\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thanawit C\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:404\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    407\u001b[0m         res_values \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32mc:\\Users\\Thanawit C\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:11481\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[1;34m(values, axis)\u001b[0m\n\u001b[0;32m  11479\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([result])\n\u001b[0;32m  11480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m> 11481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thanawit C\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    145\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Thanawit C\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[1;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[1;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[0;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[1;32mc:\\Users\\Thanawit C\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\nanops.py:1098\u001b[0m, in \u001b[0;36m_nanminmax.<locals>.reduction\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _na_for_min_count(values, axis)\n\u001b[0;32m   1095\u001b[0m values, mask \u001b[38;5;241m=\u001b[39m _get_values(\n\u001b[0;32m   1096\u001b[0m     values, skipna, fill_value_typ\u001b[38;5;241m=\u001b[39mfill_value_typ, mask\u001b[38;5;241m=\u001b[39mmask\n\u001b[0;32m   1097\u001b[0m )\n\u001b[1;32m-> 1098\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m result \u001b[38;5;241m=\u001b[39m _maybe_null_out(result, axis, mask, values\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Thanawit C\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:49\u001b[0m, in \u001b[0;36m_amin\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_minimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'Timestamp' and 'float'"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import os  \n",
    "import pyodbc  \n",
    "import pandasql as ps  \n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Paths to all cleaned stock files  \n",
    "file_paths = {  \n",
    "    'CJ_Stock': r'D:\\Data for Stock Report\\cleaned_CJ_Stock_Report.xlsx',  \n",
    "    'Daily_SO': r'D:\\Data for Stock Report\\appended_cleaned_SellOut.xlsx',  \n",
    "    'PO_HBA': r'D:\\Data for Stock Report\\cleaned_PO_pending_HBA.xlsx',  \n",
    "    'PO_Import': r'D:\\Data for Stock Report\\cleaned_PO_pending_import_party.xlsx',  \n",
    "    'Access_PO': r'D:\\Data for Stock Report\\cleaned_PO_pending_other.xlsx',  \n",
    "    'Daily_Stock_DC': r'D:\\Data for Stock Report\\cleaned_DC_daily_stock.xlsx',\n",
    "    'Master_Owner&LT': r'C:\\Users\\Thanawit C\\OneDrive - Sahamit Product Co.,Ltd\\Data for Stock Report\\COPY_MasterLeadTime.xlsx'  \n",
    "}  \n",
    "\n",
    "# Load Excel files into DataFrames\n",
    "def load_data(file_paths):\n",
    "    dataframes = {}\n",
    "    for name, path in file_paths.items():\n",
    "        try:\n",
    "            if name == \"Master_Owner&LT\":\n",
    "                # Load the specified sheet\n",
    "                owner_scm_df = pd.read_excel(path, sheet_name='All_Product', header=1) \n",
    "                \n",
    "                # Select specific columns\n",
    "                owner_scm_df_selected = owner_scm_df[['SHM_Item', 'CJ_Item', 'OwnerSCM', 'Base Lead Time (Days)']]\n",
    "                owner_scm_df_selected.rename(columns={'Base Lead Time (Days)': 'LeadTime'}, inplace=True)\n",
    "\n",
    "                # Cast CJ_Item to str\n",
    "                owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].astype(str)\n",
    "                # Clean CJ_Item column\n",
    "                owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].fillna('')\n",
    "                owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].apply(lambda x: x.split('.')[0] if '.0' in str(x) else x)\n",
    "\n",
    "                # Store the processed DataFrame\n",
    "                dataframes[name] = owner_scm_df_selected\n",
    "            else:\n",
    "                # Load other sheets  \n",
    "                sheet_name = {\n",
    "                    'CJ_Stock': 'CJ Stock',\n",
    "                    'Daily_SO': 'Pivot SO',\n",
    "                    'PO_HBA': 'Pivot HBA',  \n",
    "                    'PO_Import': 'Pivot Import',  \n",
    "                    'Access_PO': 'Pivot All PO pending',  \n",
    "                    'Daily_Stock_DC': 'Pivot_DC_stock'\n",
    "                }.get(name)  \n",
    "\n",
    "                dataframes[name] = pd.read_excel(path, sheet_name=sheet_name)\n",
    "        except Exception as e:  \n",
    "            print(f\"Error loading {name}: {e}\")\n",
    "    return dataframes\n",
    "\n",
    "# Connect to Access database and fetch product details  \n",
    "def load_access_data():  \n",
    "    access_db_path = r'D:\\DataBase Access\\SHM_TMS_001_Master_Copy.accdb'  \n",
    "    conn_str = (  \n",
    "        r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'  \n",
    "        f'DBQ={access_db_path};'  \n",
    "    )  \n",
    "    try:  \n",
    "        conn = pyodbc.connect(conn_str)  \n",
    "        print(\"Connection successful\")  \n",
    "\n",
    "        query = \"\"\"  \n",
    "        SELECT CJ_Item,  \n",
    "               SHM_Item,  \n",
    "               Description,  \n",
    "               Devision,  \n",
    "               [Group],  \n",
    "               PC_Cartons,  \n",
    "               CJ_Status,  \n",
    "               Cat,  \n",
    "               Sub_Cat,  \n",
    "               Brand  \n",
    "          FROM qry_Product_List  \n",
    "        \"\"\"\n",
    "\n",
    "        query2 = \"\"\"  \n",
    "        SELECT \n",
    "            t2.Item,\n",
    "            t2.Unit\n",
    "        FROM (\n",
    "            SELECT Item, Unit, [PO Date] AS po_date\n",
    "            FROM tbl_AllPO_Details\n",
    "            ) AS t2\n",
    "        INNER JOIN (\n",
    "            SELECT Item, MAX([PO Date]) AS max_date\n",
    "            FROM tbl_AllPO_Details\n",
    "            Group by Item\n",
    "            ) AS t1\n",
    "        ON t1.Item = t2.item AND t2.po_date = t1.max_date\n",
    "        GROUP BY t2.Item, t2.Unit\n",
    "        \"\"\"\n",
    "\n",
    "        query3 = \"\"\"  \n",
    "        SELECT \n",
    "            qry_pdl.CJ_Item,\n",
    "            qry_pdl.SHM_Item,\n",
    "            qry_pdl.Supplier_Code,  \n",
    "            qry_pdl.[Supplier Name]\n",
    "        FROM qry_Product_List AS qry_pdl\n",
    "        \"\"\"    \n",
    "\n",
    "        # Execute the queries\n",
    "        access_df = pd.read_sql(query, conn)\n",
    "        query2_df = pd.read_sql(query2, conn)\n",
    "        query3_df = pd.read_sql(query3, conn)\n",
    "\n",
    "        # merge for the unit of purchase product info\n",
    "        access_df2 = pd.merge(query3_df, query2_df, left_on='SHM_Item', right_on='Item', how='left')\n",
    "\n",
    "        conn.close()  \n",
    "        print(\"Connection to Access database is closed.\")  \n",
    "        return access_df, access_df2\n",
    "    \n",
    "    except Exception as e:  \n",
    "        print(f\"Connection failed: {e}\")  \n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "\n",
    "# Convert 'CJ_Item' to string format in all DataFrames  \n",
    "def convert_cj_item_to_string(dataframes, access_df, access_df2):\n",
    "    # Process each DataFrame in the dictionary\n",
    "    for name, df in dataframes.items():\n",
    "        if 'CJ_Item' in df.columns:\n",
    "            dataframes[name]['CJ_Item'] = df['CJ_Item'].astype(str).str.split('.').str[0]\n",
    "        if 'SHM_Item' in df.columns:\n",
    "            dataframes[name]['SHM_Item'] = df['SHM_Item'].astype(str).str.split('.').str[0]\n",
    "    \n",
    "    # Apply the same for access_df\n",
    "    if 'CJ_Item' in access_df.columns:\n",
    "        access_df['CJ_Item'] = access_df['CJ_Item'].astype(str).str.split('.').str[0]\n",
    "    \n",
    "    # Apply the same for access_df2\n",
    "    if 'CJ_Item' in access_df2.columns:\n",
    "        access_df2['CJ_Item'] = access_df2['CJ_Item'].astype(str).str.split('.').str[0]\n",
    "    \n",
    "    return access_df, access_df2\n",
    "\n",
    "# Merge all DataFrames  \n",
    "def merge_dataframes(dfs, access_df):  \n",
    "    # Merge the 'CJ_Stock' DataFrame with 'Pivot SO' DataFrame  \n",
    "    merged_df = dfs['CJ_Stock'].merge(dfs['Daily_SO'],on='CJ_Item',how='outer',suffixes=('_from-CJ', '_from-DailySO'))  \n",
    "    \n",
    "    # Continue merging with other DataFrames  \n",
    "    merged_df = merged_df.merge(dfs['PO_HBA'], on='CJ_Item', how='outer', suffixes=('', '_from-HBA'))\n",
    "    merged_df = merged_df.merge(dfs['PO_Import'], on='CJ_Item', how='outer', suffixes=('_from-HBA', '_from-Import'))\n",
    "\n",
    "    # Rename SHM_Item for clarity  \n",
    "    dfs['Access_PO'] = dfs['Access_PO'].rename(columns={'SHM_Item': 'SHM_Item_from-All_PO'})  \n",
    "\n",
    "    # Merge with Access PO DataFrame  \n",
    "    merged_df = merged_df.merge(dfs['Access_PO'], on='CJ_Item', how='outer', suffixes=('_from-Import', '_from-All_PO'))  \n",
    "    \n",
    "    # Merge with Daily Stock DC DataFrame  \n",
    "    merged_df = merged_df.merge(dfs['Daily_Stock_DC'], on='CJ_Item', how='outer', suffixes=('_from-All_PO', '_from-DailyDC'))  \n",
    "\n",
    "    # Merge with Access database DataFrame  \n",
    "    merged_df = merged_df.merge(access_df, on='CJ_Item', how='left')\n",
    "\n",
    "    # Rename columns\n",
    "    merged_df = merged_df.rename(columns={  \n",
    "        'SHM_Item': 'SHM_Item_from_qry_Product_List',\n",
    "        'Division': 'Division_CJ_stock',  \n",
    "        'Devision': 'Division_SHM'  \n",
    "    })  \n",
    "\n",
    "    # Create a new column for NPD Status by First_SO_Date \n",
    "    today = pd.to_datetime(datetime.now().date())  \n",
    "    merged_df['days_from_first_ATP'] = (today - pd.to_datetime(merged_df['First_SO_Date'])).dt.days  \n",
    "    merged_df['NPD_Status'] = np.where(merged_df['days_from_first_ATP'] <= 31, 'NPD', '-')  \n",
    "\n",
    "    # Fill in missing values for descriptive columns  \n",
    "    merged_df['Name'] = merged_df['Name'].fillna(merged_df['Description'])  \n",
    "    merged_df['Category'] = merged_df['Category'].fillna(merged_df['Cat'])  \n",
    "    merged_df['Subcate'] = merged_df['Subcate'].fillna(merged_df['Sub_Cat'])  \n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Create the 'SHM_Item' column based on priority from various sources\n",
    "def create_shm_item_column(merged_df):\n",
    "    merged_df['SHM_Item_from-All_PO'] = merged_df['SHM_Item_from-All_PO'].replace('', np.nan)\n",
    "    merged_df['SHM_Item_from-HBA'] = merged_df['SHM_Item_from-HBA'].replace('', np.nan)\n",
    "    merged_df['SHM_Item_from-Import'] = merged_df['SHM_Item_from-Import'].replace('', np.nan)\n",
    "    merged_df['SHM_Item_from_qry_Product_List'] = merged_df['SHM_Item_from_qry_Product_List'].replace('', np.nan)\n",
    "\n",
    "    merged_df['SHM_Item'] = np.where(\n",
    "        merged_df['SHM_Item_from-All_PO'].notna(), merged_df['SHM_Item_from-All_PO'],\n",
    "        np.where(merged_df['SHM_Item_from-HBA'].notna(), merged_df['SHM_Item_from-HBA'],\n",
    "                 np.where(merged_df['SHM_Item_from-Import'].notna(), merged_df['SHM_Item_from-Import'],\n",
    "                          merged_df['SHM_Item_from_qry_Product_List'].fillna('NO SHM Item in Access')))\n",
    "    )\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Fill NaN values in numeric columns with 0\n",
    "def fill_na_with_zero(df):\n",
    "    df[df.select_dtypes(include=[np.number]).columns] = df.select_dtypes(include=[np.number]).fillna(0)\n",
    "    return df\n",
    "\n",
    "def rearrange_columns(merged_df):\n",
    "    first_columns = ['CJ_Item','SHM_Item','SHM_Item_from-All_PO','SHM_Item_from-HBA','SHM_Item_from-Import','SHM_Item_from_qry_Product_List']\n",
    "    remaining_columns = [col for col in merged_df.columns if col not in first_columns]\n",
    "    final_column_order = first_columns + remaining_columns\n",
    "    merged_df = merged_df[final_column_order]\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Create new column to sum ALL PO Pending\n",
    "def calculate_totals(merged_df):\n",
    "    dc_columns = [1, 2, 4]\n",
    "    for dc in dc_columns:\n",
    "        merged_df[f'Total-PO_qty_to_DC{dc}'] = (\n",
    "            merged_df[f'PO_Qty_to_DC{dc}'] +\n",
    "            merged_df[f'PO_Qty_to_DC{dc}_from-Import'] + \n",
    "            merged_df[f'PO_Qty_to_DC{dc}_from-HBA'])\n",
    "\n",
    "        # Calculate %Ratio with error handling\n",
    "        merged_df[f'%Ratio_AvgSalesQty90D_DC{dc}'] = (\n",
    "            merged_df[f'DC{dc}_AvgSaleQty90D'] / merged_df['Total_AvgSaleQty90D'].replace(0, 1)\n",
    "        ).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    # Calculate Total Remain Stock\n",
    "    merged_df['Remain_StockQty_AllDC'] = merged_df['DC1_Remain_StockQty'] + merged_df['DC2_Remain_StockQty'] + merged_df['DC4_Remain_StockQty']\n",
    "    merged_df['Remain_StockValue_AllDC'] = merged_df['DC1_Remain_StockValue'] + merged_df['DC2_Remain_StockValue'] + merged_df['DC4_Remain_StockValue']\n",
    "\n",
    "    # Calculate SO Qty\n",
    "    for dc in dc_columns:\n",
    "        merged_df[f'DC{dc}_SO_Last30D'] = round(merged_df['SO_Qty_last30D'] * merged_df[f'%Ratio_AvgSalesQty90D_DC{dc}'])\n",
    "        merged_df[f'DC{dc}_SO_Last7D'] = round(merged_df['SO_Qty_last7D'] * merged_df[f'%Ratio_AvgSalesQty90D_DC{dc}'])\n",
    "\n",
    "        merged_df[f'DC{dc}_AvgSaleQty30D'] = merged_df[f'DC{dc}_SO_Last30D'] / 30\n",
    "        merged_df[f'DC{dc}_AvgSaleQty7D'] = merged_df[f'DC{dc}_SO_Last7D'] / 7\n",
    "\n",
    "    merged_df['Total_AvgSaleQty30D'] = merged_df[[f'DC{dc}_AvgSaleQty30D' for dc in dc_columns]].sum(axis=1)\n",
    "    merged_df['Total_AvgSaleQty7D'] = merged_df[[f'DC{dc}_AvgSaleQty7D' for dc in dc_columns]].sum(axis=1)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Simplify DOH calculation\n",
    "def calculate_DOH(stock_value, avg_cogs):\n",
    "    return np.where(\n",
    "        (stock_value != 0) & (avg_cogs == 0), np.inf,\n",
    "        np.where((stock_value == 0) & (avg_cogs == 0), 0, stock_value / avg_cogs)\n",
    "    )\n",
    "\n",
    "# Simplified DOH calculations for various stock locations\n",
    "def apply_doh_calculations(merged_df):\n",
    "    merged_df['Current_DOH_All_DC'] = calculate_DOH(merged_df['Remain_StockQty_AllDC'], merged_df['Total_AvgSaleQty90D'])\n",
    "    merged_df['Current_DC1_DOH'] = calculate_DOH(merged_df['DC1_Remain_StockQty'], merged_df['DC1_AvgSaleQty90D'])\n",
    "    merged_df['Current_DC2_DOH'] = calculate_DOH(merged_df['DC2_Remain_StockQty'], merged_df['DC2_AvgSaleQty90D'])\n",
    "    merged_df['Current_DC4_DOH'] = calculate_DOH(merged_df['DC4_Remain_StockQty'], merged_df['DC4_AvgSaleQty90D'])\n",
    "\n",
    "    # Create new col for calculate\n",
    "    merged_df['Total-PO_qty_to_DC'] = merged_df['Total-PO_qty_to_DC1'] + merged_df['Total-PO_qty_to_DC2'] + merged_df['Total-PO_qty_to_DC4']\n",
    "    merged_df['Current_DOH(Stock+PO)_All_DC'] = calculate_DOH(merged_df['Remain_StockQty_AllDC'] + merged_df['Total-PO_qty_to_DC'], merged_df['Total_AvgSaleQty90D'])\n",
    "    merged_df['DC1_DOH(Stock+PO)'] = calculate_DOH(merged_df['DC1_Remain_StockQty'] + merged_df['Total-PO_qty_to_DC1'], merged_df['DC1_AvgSaleQty90D'])\n",
    "    merged_df['DC2_DOH(Stock+PO)'] = calculate_DOH(merged_df['DC2_Remain_StockQty'] + merged_df['Total-PO_qty_to_DC2'], merged_df['DC2_AvgSaleQty90D'])\n",
    "    merged_df['DC4_DOH(Stock+PO)'] = calculate_DOH(merged_df['DC4_Remain_StockQty'] + merged_df['Total-PO_qty_to_DC4'], merged_df['DC4_AvgSaleQty90D'])\n",
    "\n",
    "\n",
    "    merged_df['Min_delivery_date_to_DC1'] = merged_df[['Min_del_date_to_DC1', 'Min_del_date_to_DC1_from-Import', 'Min_del_date_to_DC1_from-HBA']].min(axis=1)\n",
    "    merged_df['Min_delivery_date_to_DC2'] = merged_df[['Min_del_date_to_DC2', 'Min_del_date_to_DC2_from-Import', 'Min_del_date_to_DC2_from-HBA']].min(axis=1)\n",
    "    merged_df['Min_delivery_date_to_DC4'] = merged_df[['Min_del_date_to_DC4','Min_del_date_to_DC4_from-Import','Min_del_date_to_DC4_from-HBA']].min(axis=1)\n",
    "    merged_df['Min_delivery_date_to_DC'] = merged_df[['Min_delivery_date_to_DC1', 'Min_delivery_date_to_DC2','Min_delivery_date_to_DC4']].min(axis=1)\n",
    "\n",
    "    # Calculate Stock cover date as a date time format\n",
    "    current_date = pd.to_datetime(datetime.now().date())\n",
    "    # Define maximum value for calculation\n",
    "    max_doh_value = 1825\n",
    "\n",
    "    # Set Current DOH if exceeding the max DOH, then = infinite\n",
    "    merged_df['Current_DOH_All_DC'] = np.where(merged_df['Current_DOH_All_DC'] > max_doh_value, np.inf, merged_df['Current_DOH_All_DC'])\n",
    "    merged_df['Current_DC1_DOH'] = np.where(merged_df['Current_DC1_DOH'] > max_doh_value, np.inf, merged_df['Current_DC1_DOH'])\n",
    "    merged_df['Current_DC2_DOH'] = np.where(merged_df['Current_DC2_DOH'] > max_doh_value, np.inf, merged_df['Current_DC2_DOH'])\n",
    "    merged_df['Current_DC4_DOH'] = np.where(merged_df['Current_DC4_DOH'] > max_doh_value, np.inf, merged_df['Current_DC4_DOH'])\n",
    "\n",
    "    # Also set DOH of Stock+PO to inf\n",
    "    merged_df['Current_DOH(Stock+PO)_All_DC'] = np.where(merged_df['Current_DOH(Stock+PO)_All_DC'] > max_doh_value, np.inf, merged_df['Current_DOH(Stock+PO)_All_DC'])\n",
    "    merged_df['DC1_DOH(Stock+PO)'] = np.where(merged_df['DC1_DOH(Stock+PO)'] > max_doh_value, np.inf, merged_df['DC1_DOH(Stock+PO)'])\n",
    "    merged_df['DC2_DOH(Stock+PO)'] = np.where(merged_df['DC2_DOH(Stock+PO)'] > max_doh_value, np.inf, merged_df['DC2_DOH(Stock+PO)'])\n",
    "    merged_df['DC4_DOH(Stock+PO)'] = np.where(merged_df['DC4_DOH(Stock+PO)'] > max_doh_value, np.inf, merged_df['DC4_DOH(Stock+PO)'])\n",
    "\n",
    "    # Create new columns and initialize with NaT\n",
    "    merged_df['Total_Store_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock_All_DC_Cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock+PO_All_DC_Cover_to_date'] = pd.NaT\n",
    "    merged_df['Store_DC1_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock_DC1_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock+PO_DC1_cover_to_date'] = pd.NaT\n",
    "    merged_df['Store_DC2_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock_DC2_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock+PO_DC2_cover_to_date'] = pd.NaT\n",
    "    merged_df['Store_DC4_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock_DC4_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock+PO_DC4_cover_to_date'] = pd.NaT\n",
    "\n",
    "\n",
    "    # Loop through each row to handle calculations\n",
    "    for index, row in merged_df.iterrows():\n",
    "        # Process Total Stores DOH\n",
    "        all_store_doh = row['Total_DOHStore']\n",
    "        if pd.notnull(all_store_doh) and all_store_doh > 0 and not np.isinf(all_store_doh) and all_store_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Total_Store_cover_to_date'] = current_date + pd.to_timedelta(all_store_doh, unit='d')\n",
    "\n",
    "        # Process for Current DOH All DC\n",
    "        Current_DOH_All_DC = row['Current_DOH_All_DC']\n",
    "        if pd.notnull(Current_DOH_All_DC) and Current_DOH_All_DC > 0 and not np.isinf(Current_DOH_All_DC) and Current_DOH_All_DC <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock_All_DC_Cover_to_date'] = current_date + pd.to_timedelta(Current_DOH_All_DC, unit='d')\n",
    "\n",
    "        # Process for Current Stock + PO\n",
    "        doh_all_dc_plus_po = row['Current_DOH(Stock+PO)_All_DC']\n",
    "        if pd.notnull(doh_all_dc_plus_po) and doh_all_dc_plus_po > 0 and not np.isinf(doh_all_dc_plus_po) and doh_all_dc_plus_po <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock+PO_All_DC_Cover_to_date'] = current_date + pd.to_timedelta(doh_all_dc_plus_po, unit='d')\n",
    "\n",
    "        # Process Stores DC1 DOH\n",
    "        DC1_store_doh = row['DC1_DOHStore']\n",
    "        if pd.notnull(DC1_store_doh) and DC1_store_doh > 0 and not np.isinf(DC1_store_doh) and DC1_store_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Store_DC1_cover_to_date'] = current_date + pd.to_timedelta(DC1_store_doh, unit='d')\n",
    "\n",
    "        # Process for Current DC1 DOH\n",
    "        current_dc1_doh = row['Current_DC1_DOH']\n",
    "        if pd.notnull(current_dc1_doh) and current_dc1_doh > 0 and not np.isinf(current_dc1_doh) and current_dc1_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock_DC1_cover_to_date'] = current_date + pd.to_timedelta(current_dc1_doh, unit='d')\n",
    "\n",
    "        # Process for Current Stock + PO DC1\n",
    "        doh_dc1_plus_po = row['DC1_DOH(Stock+PO)']\n",
    "        if pd.notnull(doh_dc1_plus_po) and doh_dc1_plus_po > 0 and not np.isinf(doh_dc1_plus_po) and doh_dc1_plus_po <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock+PO_DC1_cover_to_date'] = current_date + pd.to_timedelta(doh_dc1_plus_po, unit='d')\n",
    "\n",
    "        # Process Stores DC2 DOH\n",
    "        DC2_store_doh = row['DC2_DOHStore']\n",
    "        if pd.notnull(DC2_store_doh) and DC2_store_doh > 0 and not np.isinf(DC2_store_doh) and DC2_store_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Store_DC2_cover_to_date'] = current_date + pd.to_timedelta(DC2_store_doh, unit='d')\n",
    "\n",
    "        # Process for Current DC2 DOH\n",
    "        current_dc2_doh = row['Current_DC2_DOH']\n",
    "        if pd.notnull(current_dc2_doh) and current_dc2_doh > 0 and not np.isinf(current_dc2_doh) and current_dc2_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock_DC2_cover_to_date'] = current_date + pd.to_timedelta(current_dc2_doh, unit='d')\n",
    "\n",
    "        # Process for Current Stock + PO DC2\n",
    "        doh_dc2_plus_po = row['DC2_DOH(Stock+PO)']\n",
    "        if pd.notnull(doh_dc2_plus_po) and doh_dc2_plus_po > 0 and not np.isinf(doh_dc2_plus_po) and doh_dc2_plus_po <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock+PO_DC2_cover_to_date'] = current_date + pd.to_timedelta(doh_dc2_plus_po, unit='d')\n",
    "\n",
    "        # Process Stores DC4 DOH\n",
    "        DC4_store_doh = row['DC4_DOHStore']\n",
    "        if pd.notnull(DC4_store_doh) and DC4_store_doh > 0 and not np.isinf(DC4_store_doh) and DC4_store_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Store_DC4_cover_to_date'] = current_date + pd.to_timedelta(DC4_store_doh, unit='d')            \n",
    "\n",
    "        # Process for Current DC4 DOH\n",
    "        current_dc4_doh = row['Current_DC4_DOH']\n",
    "        if pd.notnull(current_dc4_doh) and current_dc4_doh > 0 and not np.isinf(current_dc4_doh) and current_dc4_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock_DC4_cover_to_date'] = current_date + pd.to_timedelta(current_dc4_doh, unit='d')\n",
    "\n",
    "        # Process for Current Stock + PO DC4\n",
    "        doh_dc4_plus_po = row['DC4_DOH(Stock+PO)']\n",
    "        if pd.notnull(doh_dc4_plus_po) and doh_dc4_plus_po > 0 and not np.isinf(doh_dc4_plus_po) and doh_dc4_plus_po <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock+PO_DC4_cover_to_date'] = current_date + pd.to_timedelta(doh_dc4_plus_po, unit='d')\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    dataframes = load_data(file_paths)\n",
    "    access_df, access_df2 = load_access_data()\n",
    "    access_df, access_df2 = convert_cj_item_to_string(dataframes, access_df, access_df2)\n",
    "    \n",
    "    merged_df = merge_dataframes(dataframes, access_df)\n",
    "    merged_df = create_shm_item_column(merged_df)\n",
    "\n",
    "    # Merge with access_df again to get the correct supplier name\n",
    "    merged_df = pd.merge(merged_df, access_df2, on=['CJ_Item', 'SHM_Item'], how='left')\n",
    "    # Merge with master Owner and lead time to get owner name and lead time\n",
    "    merged_df = merged_df.merge(dataframes['Master_Owner&LT'],on=['SHM_Item','CJ_Item'],how='left')\n",
    "    merged_df = merged_df.fillna({'OwnerSCM': 'No data', 'LeadTime': 'No data'})\n",
    "\n",
    "    merged_df = fill_na_with_zero(merged_df)\n",
    "    merged_df = rearrange_columns(merged_df)\n",
    "    merged_df = calculate_totals(merged_df)\n",
    "    merged_df = apply_doh_calculations(merged_df)\n",
    "\n",
    "   # Execute SQL query\n",
    "    query = \"\"\"\n",
    "    SELECT Division_SHM,\n",
    "        OwnerSCM,\n",
    "        [Supplier Name],\n",
    "        SHM_Item,\n",
    "        CJ_Item,\n",
    "        Name,\n",
    "        Category,\n",
    "        Brand,\n",
    "        LeadTime,\n",
    "        Status,\n",
    "        [Group],\n",
    "        Unit,\n",
    "        PC_Cartons,\n",
    "        First_SO_Date,\n",
    "        NPD_Status,\n",
    "        Total_ScmAssort,\n",
    "        Total_OOSAssort,\n",
    "        Total_CountOKROOS,\n",
    "        Total_PercOOS,\n",
    "        Total_StoreStockQty,\n",
    "        Total_DOHStore,\n",
    "        Total_Store_cover_to_date,\n",
    "        Total_AvgSaleQty90D,\n",
    "        Total_AvgSaleQty30D,\n",
    "        Total_AvgSaleQty7D,\n",
    "        SO_Qty_last7D,\n",
    "        Remain_StockQty_AllDC,\n",
    "        Current_DOH_All_DC,\n",
    "        Stock_All_DC_Cover_to_date,\n",
    "        [Total-PO_qty_to_DC],\n",
    "        Min_delivery_date_to_DC,\n",
    "        [Current_DOH(Stock+PO)_All_DC],\n",
    "        [Stock+PO_All_DC_Cover_to_date],\n",
    "        DC1_ScmAssort,\n",
    "        DC1_OOSAssort,\n",
    "        DC1_CountOKROOS,\n",
    "        DC1_PercOOS,\n",
    "        DC1_StoreStockQty,\n",
    "        DC1_DOHStore,\n",
    "        Store_DC1_cover_to_date,\n",
    "        DC1_AvgSaleQty90D,\n",
    "        DC1_AvgSaleQty30D,\n",
    "        DC1_AvgSaleQty7D,\n",
    "        [%Ratio_AvgSalesQty90D_DC1],\n",
    "        DC1_Remain_StockQty,\n",
    "        Current_DC1_DOH,\n",
    "        Stock_DC1_cover_to_date,\n",
    "        [Total-PO_qty_to_DC1],\n",
    "        Min_delivery_date_to_DC1,\n",
    "        [DC1_DOH(Stock+PO)],\n",
    "        [Stock+PO_DC1_cover_to_date],\n",
    "        DC2_ScmAssort,\n",
    "        DC2_OOSAssort,\n",
    "        DC2_CountOKROOS,\n",
    "        DC2_PercOOS,\n",
    "        DC2_StoreStockQty,\n",
    "        DC2_DOHStore,\n",
    "        Store_DC2_cover_to_date,\n",
    "        DC2_AvgSaleQty90D,\n",
    "        DC2_AvgSaleQty30D,\n",
    "        DC2_AvgSaleQty7D,\n",
    "        [%Ratio_AvgSalesQty90D_DC2],\n",
    "        DC2_Remain_StockQty,\n",
    "        Current_DC2_DOH,\n",
    "        Stock_DC2_cover_to_date,\n",
    "        [Total-PO_qty_to_DC2],\n",
    "        Min_delivery_date_to_DC2,\n",
    "        [DC2_DOH(Stock+PO)],\n",
    "        [Stock+PO_DC2_cover_to_date],\n",
    "        DC4_ScmAssort,\n",
    "        DC4_OOSAssort,\n",
    "        DC4_CountOKROOS,\n",
    "        DC4_PercOOS,\n",
    "        DC4_StoreStockQty,\n",
    "        DC4_DOHStore,\n",
    "        Store_DC4_cover_to_date,\n",
    "        DC4_AvgSaleQty90D,\n",
    "        DC4_AvgSaleQty30D,\n",
    "        DC4_AvgSaleQty7D,\n",
    "        [%Ratio_AvgSalesQty90D_DC4],\n",
    "        DC4_Remain_StockQty,\n",
    "        Current_DC4_DOH,\n",
    "        Stock_DC4_cover_to_date,\n",
    "        [Total-PO_qty_to_DC4],\n",
    "        Min_delivery_date_to_DC4,\n",
    "        [DC4_DOH(Stock+PO)],\n",
    "        [Stock+PO_DC4_cover_to_date]\n",
    "    FROM merged_df\n",
    "    WHERE ([Group] != 'Discontinuous' or [Group] IS NULL)\n",
    "    \"\"\"\n",
    "\n",
    "    result_df = ps.sqldf(query, locals())\n",
    "    \n",
    "\n",
    "    # Rename All column for store to display a data this column is freezed\n",
    "    columns_to_rename = {\n",
    "        'Status': 'CJ_Status',\n",
    "        'Group': 'SHM_Status',\n",
    "        'Unit': 'Unit_of_Purchase',\n",
    "        'LeadTime':'LeadTime(Days)',\n",
    "        'Total_OOSAssort':'Total_ActiveAssort',\n",
    "        'Total_CountOKROOS':'Total_StoreOOS',\n",
    "        'Total_PercOOS':'Total_%StoreOOS'\n",
    "        }\n",
    "    \n",
    "    # Rename the columns\n",
    "    result_df.rename(columns=columns_to_rename, inplace=True)\n",
    "    renamed_columns = [columns_to_rename[old_name] for old_name in columns_to_rename if old_name in result_df.columns]\n",
    "    print(f\"The following columns have been renamed: {columns_to_rename}\")\n",
    "\n",
    "\n",
    "    # Filter to show the data only all numeric column > 0\n",
    "    numeric_cols = result_df.select_dtypes(include=[np.number]).columns\n",
    "    result_df = result_df[(result_df[numeric_cols] != 0).any(axis=1)]\n",
    "\n",
    "    # Format the date columns to display in Excel\n",
    "    date_columns = [\n",
    "        'First_SO_Date',\n",
    "        'Min_delivery_date_to_DC',\n",
    "        'Min_delivery_date_to_DC1',\n",
    "        'Min_delivery_date_to_DC2',\n",
    "        'Min_delivery_date_to_DC4',\n",
    "        'Total_Store_cover_to_date',\n",
    "        'Store_DC1_cover_to_date',\n",
    "        'Store_DC2_cover_to_date',\n",
    "        'Store_DC4_cover_to_date',\n",
    "        'Stock_All_DC_Cover_to_date',\n",
    "        'Stock_DC1_cover_to_date',\n",
    "        'Stock_DC2_cover_to_date',\n",
    "        'Stock_DC4_cover_to_date',\n",
    "        'Stock+PO_All_DC_Cover_to_date',\n",
    "        'Stock+PO_DC1_cover_to_date',\n",
    "        'Stock+PO_DC2_cover_to_date',\n",
    "        'Stock+PO_DC4_cover_to_date'\n",
    "    ]\n",
    "\n",
    "    for col in date_columns:\n",
    "        if col in result_df.columns:\n",
    "            result_df[col] = pd.to_datetime(result_df[col], errors='coerce')  # Ensure it's in datetime format\n",
    "    \n",
    "\n",
    "    # Drop Duplicate rows when all data in each columns is duplicated except in the []\n",
    "    column_to_check = result_df.columns.difference(['SHM_Item','SHM_Status'])  # get all column except in []\n",
    "    deduplicated_df = result_df.drop_duplicates(subset=column_to_check)\n",
    "\n",
    "\n",
    "    # Drop SHM_Item = No shm item in access and Save deduplicated_df to excel\n",
    "    deduplicated_df = deduplicated_df[deduplicated_df['SHM_Item'] != 'NO SHM Item in Access']\n",
    "    deduplicated_df.to_excel(r'D:\\Data for Stock Report\\Data_for_OOS_Log\\3.Mar_24-03-2025.xlsx', index=False)\n",
    "\n",
    "    # Handle duplicates 'CJ_Item' by using groupby\n",
    "    def replace_cj_duplicate(group):\n",
    "        # For each numeric check for duplicates\n",
    "        for col in numeric_cols:\n",
    "            if col == 'PC_Cartons':\n",
    "                continue  # Skip when column = pc_carton\n",
    "            if group[col].nunique() == 1:  # All values are the same in this group\n",
    "                group.iloc[1:, group.columns.get_loc(col)] = 0  # Replace all but keep the first row\n",
    "        \n",
    "        # Filter these 2 columns if both columns are 0 then drop the row, keep the first row\n",
    "        group = group[(group[['Remain_StockQty_AllDC', 'Total-PO_qty_to_DC']].ne(0).any(axis=1)) | (group.index == group.index.min())]\n",
    "\n",
    "        return group\n",
    "    \n",
    "    # Apply this function to each group of 'CJ_Item'\n",
    "    deduplicated_df = deduplicated_df.groupby('CJ_Item').apply(replace_cj_duplicate).reset_index(drop=True)\n",
    "\n",
    "    # Rename column for converting QTY to BOX QTY\n",
    "    column_rename_mapping = {\n",
    "        'Total_AvgSaleQty90D': 'Total_AvgSaleCTN_Last90D',\n",
    "        'Total_AvgSaleQty30D': 'Total_AvgSaleCTN_Last30D',\n",
    "        'Total_AvgSaleQty7D': 'Total_AvgSaleCTN_Last7D',\n",
    "        'DC1_AvgSaleQty90D': 'DC1_AvgSaleCTN_Last90Days',\n",
    "        'DC1_AvgSaleQty30D': 'DC1_AvgSaleCTN_Last30Days',\n",
    "        'DC1_AvgSaleQty7D': 'DC1_AvgSaleCTN_Last7Days',\n",
    "        'DC2_AvgSaleQty90D': 'DC2_AvgSaleCTN_Last90Days',\n",
    "        'DC2_AvgSaleQty30D': 'DC2_AvgSaleCTN_Last30Days',\n",
    "        'DC2_AvgSaleQty7D': 'DC2_AvgSaleCTN_Last7Days',\n",
    "        'DC4_AvgSaleQty90D': 'DC4_AvgSaleCTN_Last90Days',\n",
    "        'DC4_AvgSaleQty30D': 'DC4_AvgSaleCTN_Last30Days',\n",
    "        'DC4_AvgSaleQty7D': 'DC4_AvgSaleCTN_Last7Days',\n",
    "        'Total_StoreStockQty': 'Total_StoreStockCTN',\n",
    "        'DC1_StoreStockQty': 'DC1_StoreStockCTN',\n",
    "        'DC2_StoreStockQty': 'DC2_StoreStockCTN',\n",
    "        'DC4_StoreStockQty': 'DC4_StoreStockCTN',\n",
    "        'SO_Qty_last7D': 'SO_CTN_last7D',\n",
    "        'Remain_StockQty_AllDC': 'Remain_CTN_AllDC',\n",
    "        'DC1_Remain_StockQty': 'DC1_Remain_CTN',\n",
    "        'DC2_Remain_StockQty': 'DC2_Remain_CTN',\n",
    "        'DC4_Remain_StockQty': 'DC4_Remain_CTN',\n",
    "        'Total-PO_qty_to_DC': 'Total-CTN_to_DC',\n",
    "        'Total-PO_qty_to_DC1': 'Total-CTN_to_DC1',\n",
    "        'Total-PO_qty_to_DC2': 'Total-CTN_to_DC2',\n",
    "        'Total-PO_qty_to_DC4': 'Total-CTN_to_DC4'\n",
    "    }\n",
    "\n",
    "    # Process the rename\n",
    "    modified_df = deduplicated_df.rename(columns=column_rename_mapping)\n",
    "\n",
    "    # List of new columns that were just renamed\n",
    "    renamed_columns = list(column_rename_mapping.values())\n",
    "    \n",
    "    # Loop through renamed column to perform division by PC_Cartons\n",
    "    for col in renamed_columns:\n",
    "        modified_df[col] = np.where(\n",
    "            modified_df['PC_Cartons'] == 0,0,\n",
    "            modified_df[col] / modified_df['PC_Cartons']\n",
    "        )\n",
    "\n",
    "    # Define current date and save path \n",
    "    current_date = datetime.now().strftime('%d-%m-%Y')\n",
    "    save_directory = r'D:\\Data for Stock Report\\Completed Daily Stock Report'\n",
    "    file_name = f\"Sahamit Daily Stock Report_{current_date}.xlsx\"\n",
    "    save_path = os.path.join(save_directory, file_name)\n",
    "\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    # Save the merged DataFrame\n",
    "    try: \n",
    "        with pd.ExcelWriter(save_path, mode='w') as writer:\n",
    "            deduplicated_df.to_excel(writer, sheet_name='Data by Qty', index=False)\n",
    "            modified_df.to_excel(writer, sheet_name='Data by Cartons',index=False)\n",
    "\n",
    "            # For checking Raw data\n",
    "            #merged_df.to_excel(writer, sheet_name='Raw Data', index=False)\n",
    "            #result_df.to_excel(writer, sheet_name='after query', index=False)\n",
    "\n",
    "            # Load All sheet from Raw_PO_pending and save them together\n",
    "            existing_file_path = r'D:\\Data for Stock Report\\Raw_PO_pending.xlsx'\n",
    "            if os.path.exists(existing_file_path):\n",
    "                workbook = load_workbook(existing_file_path)\n",
    "                for sheet_name in workbook.sheetnames:\n",
    "                    existing_df = pd.read_excel(existing_file_path,sheet_name=sheet_name)\n",
    "                    existing_df.to_excel(writer,sheet_name=sheet_name,index=False)\n",
    "            else:\n",
    "                print(f\"File not found: {existing_file_path}\")\n",
    "\n",
    "        print(f\"Data merged and saved successfully at {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
