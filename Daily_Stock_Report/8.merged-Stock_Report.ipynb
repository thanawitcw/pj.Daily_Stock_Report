{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  owner_scm_df_selected.rename(columns={'Base Lead Time (Days)': 'LeadTime'}, inplace=True)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].astype(str)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].fillna('')\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].apply(lambda x: x.split('.')[0] if '.0' in str(x) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:109: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  access_df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:110: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  query2_df = pd.read_sql(query2, conn)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:111: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  query3_df = pd.read_sql(query3, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to Access database is closed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:262: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Current_DOH(Stock+PO)_All_DC'] = calculate_DOH(merged_df['Remain_StockQty_AllDC'] + merged_df['Total-PO_qty_to_DC'], merged_df['Total_AvgSaleQty90D'])\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:263: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['DC1_DOH(Stock+PO)'] = calculate_DOH(merged_df['DC1_Remain_StockQty'] + merged_df['Total-PO_qty_to_DC1'], merged_df['DC1_AvgSaleQty90D'])\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:264: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['DC2_DOH(Stock+PO)'] = calculate_DOH(merged_df['DC2_Remain_StockQty'] + merged_df['Total-PO_qty_to_DC2'], merged_df['DC2_AvgSaleQty90D'])\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['DC4_DOH(Stock+PO)'] = calculate_DOH(merged_df['DC4_Remain_StockQty'] + merged_df['Total-PO_qty_to_DC4'], merged_df['DC4_AvgSaleQty90D'])\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:268: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Min_delivery_date_to_DC1'] = merged_df[['Min_del_date_to_DC1', 'Min_del_date_to_DC1_from-Import', 'Min_del_date_to_DC1_from-HBA']].min(axis=1)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:269: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Min_delivery_date_to_DC2'] = merged_df[['Min_del_date_to_DC2', 'Min_del_date_to_DC2_from-Import', 'Min_del_date_to_DC2_from-HBA']].min(axis=1)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:270: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Min_delivery_date_to_DC4'] = merged_df[['Min_del_date_to_DC4','Min_del_date_to_DC4_from-Import','Min_del_date_to_DC4_from-HBA']].min(axis=1)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:271: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Min_delivery_date_to_DC'] = merged_df[['Min_delivery_date_to_DC1', 'Min_delivery_date_to_DC2','Min_delivery_date_to_DC4']].min(axis=1)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:291: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Total_Store_cover_to_date'] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:292: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Stock_All_DC_Cover_to_date'] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:293: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Stock+PO_All_DC_Cover_to_date'] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:294: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Store_DC1_cover_to_date'] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:295: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Stock_DC1_cover_to_date'] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:296: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Stock+PO_DC1_cover_to_date'] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:297: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Store_DC2_cover_to_date'] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:298: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Stock_DC2_cover_to_date'] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:299: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Stock+PO_DC2_cover_to_date'] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:300: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Store_DC4_cover_to_date'] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:301: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Stock_DC4_cover_to_date'] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:302: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Stock+PO_DC4_cover_to_date'] = pd.NaT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following columns have been renamed: {'Status': 'CJ_Status', 'Group': 'SHM_Status', 'Unit': 'Unit_of_Purchase', 'LeadTime': 'LeadTime(Days)', 'Total_OOSAssort': 'Total_ActiveAssort', 'Total_CountOKROOS': 'Total_StoreOOS', 'Total_PercOOS': 'Total_%StoreOOS'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_16168\\1838647232.py:556: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  deduplicated_df = deduplicated_df.groupby('CJ_Item').apply(replace_cj_duplicate).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data merged and saved successfully at D:\\Data for Stock Report\\Completed Daily Stock Report\\Sahamit Daily Stock Report_10-03-2025.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import os  \n",
    "import pyodbc  \n",
    "import pandasql as ps  \n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Paths to all cleaned stock files  \n",
    "file_paths = {  \n",
    "    'CJ_Stock': r'D:\\Data for Stock Report\\cleaned_CJ_Stock_Report.xlsx',  \n",
    "    'Daily_SO': r'D:\\Data for Stock Report\\appended_cleaned_SellOut.xlsx',  \n",
    "    'PO_HBA': r'D:\\Data for Stock Report\\cleaned_PO_pending_HBA.xlsx',  \n",
    "    'PO_Import': r'D:\\Data for Stock Report\\cleaned_PO_pending_import_party.xlsx',  \n",
    "    'Access_PO': r'D:\\Data for Stock Report\\cleaned_PO_pending_other.xlsx',  \n",
    "    'Daily_Stock_DC': r'D:\\Data for Stock Report\\cleaned_DC_daily_stock.xlsx',\n",
    "    'Master_Owner&LT': r'C:\\Users\\Thanawit C\\OneDrive - Sahamit Product Co.,Ltd\\Data for Stock Report\\COPY_MasterLeadTime.xlsx'  \n",
    "}  \n",
    "\n",
    "# Load Excel files into DataFrames\n",
    "def load_data(file_paths):\n",
    "    dataframes = {}\n",
    "    for name, path in file_paths.items():\n",
    "        try:\n",
    "            if name == \"Master_Owner&LT\":\n",
    "                # Load the specified sheet\n",
    "                owner_scm_df = pd.read_excel(path, sheet_name='All_Product', header=1) \n",
    "                \n",
    "                # Select specific columns\n",
    "                owner_scm_df_selected = owner_scm_df[['SHM_Item', 'CJ_Item', 'OwnerSCM', 'Base Lead Time (Days)']]\n",
    "                owner_scm_df_selected.rename(columns={'Base Lead Time (Days)': 'LeadTime'}, inplace=True)\n",
    "\n",
    "                # Cast CJ_Item to str\n",
    "                owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].astype(str)\n",
    "                # Clean CJ_Item column\n",
    "                owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].fillna('')\n",
    "                owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].apply(lambda x: x.split('.')[0] if '.0' in str(x) else x)\n",
    "\n",
    "                # Store the processed DataFrame\n",
    "                dataframes[name] = owner_scm_df_selected\n",
    "            else:\n",
    "                # Load other sheets  \n",
    "                sheet_name = {\n",
    "                    'CJ_Stock': 'CJ Stock',\n",
    "                    'Daily_SO': 'Pivot SO',\n",
    "                    'PO_HBA': 'Pivot HBA',  \n",
    "                    'PO_Import': 'Pivot Import',  \n",
    "                    'Access_PO': 'Pivot All PO pending',  \n",
    "                    'Daily_Stock_DC': 'Pivot_DC_stock'\n",
    "                }.get(name)  \n",
    "\n",
    "                dataframes[name] = pd.read_excel(path, sheet_name=sheet_name)\n",
    "        except Exception as e:  \n",
    "            print(f\"Error loading {name}: {e}\")\n",
    "    return dataframes\n",
    "\n",
    "# Connect to Access database and fetch product details  \n",
    "def load_access_data():  \n",
    "    access_db_path = r'D:\\DataBase Access\\SHM_TMS_001_Master_Copy.accdb'  \n",
    "    conn_str = (  \n",
    "        r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'  \n",
    "        f'DBQ={access_db_path};'  \n",
    "    )  \n",
    "    try:  \n",
    "        conn = pyodbc.connect(conn_str)  \n",
    "        print(\"Connection successful\")  \n",
    "\n",
    "        query = \"\"\"  \n",
    "        SELECT CJ_Item,  \n",
    "               SHM_Item,  \n",
    "               Description,  \n",
    "               Devision,  \n",
    "               [Group],  \n",
    "               PC_Cartons,  \n",
    "               CJ_Status,  \n",
    "               Cat,  \n",
    "               Sub_Cat,  \n",
    "               Brand  \n",
    "          FROM qry_Product_List  \n",
    "        \"\"\"\n",
    "\n",
    "        query2 = \"\"\"  \n",
    "        SELECT \n",
    "            t2.Item,\n",
    "            t2.Unit\n",
    "        FROM (\n",
    "            SELECT Item, Unit, [PO Date] AS po_date\n",
    "            FROM tbl_AllPO_Details\n",
    "            ) AS t2\n",
    "        INNER JOIN (\n",
    "            SELECT Item, MAX([PO Date]) AS max_date\n",
    "            FROM tbl_AllPO_Details\n",
    "            Group by Item\n",
    "            ) AS t1\n",
    "        ON t1.Item = t2.item AND t2.po_date = t1.max_date\n",
    "        GROUP BY t2.Item, t2.Unit\n",
    "        \"\"\"\n",
    "\n",
    "        query3 = \"\"\"  \n",
    "        SELECT \n",
    "            qry_pdl.CJ_Item,\n",
    "            qry_pdl.SHM_Item,\n",
    "            qry_pdl.Supplier_Code,  \n",
    "            qry_pdl.[Supplier Name]\n",
    "        FROM qry_Product_List AS qry_pdl\n",
    "        \"\"\"    \n",
    "\n",
    "        # Execute the queries\n",
    "        access_df = pd.read_sql(query, conn)\n",
    "        query2_df = pd.read_sql(query2, conn)\n",
    "        query3_df = pd.read_sql(query3, conn)\n",
    "\n",
    "        # merge for the unit of purchase product info\n",
    "        access_df2 = pd.merge(query3_df, query2_df, left_on='SHM_Item', right_on='Item', how='left')\n",
    "\n",
    "        conn.close()  \n",
    "        print(\"Connection to Access database is closed.\")  \n",
    "        return access_df, access_df2\n",
    "    \n",
    "    except Exception as e:  \n",
    "        print(f\"Connection failed: {e}\")  \n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "\n",
    "# Convert 'CJ_Item' to string format in all DataFrames  \n",
    "def convert_cj_item_to_string(dataframes, access_df, access_df2):\n",
    "    # Process each DataFrame in the dictionary\n",
    "    for name, df in dataframes.items():\n",
    "        if 'CJ_Item' in df.columns:\n",
    "            dataframes[name]['CJ_Item'] = df['CJ_Item'].astype(str).str.split('.').str[0]\n",
    "        if 'SHM_Item' in df.columns:\n",
    "            dataframes[name]['SHM_Item'] = df['SHM_Item'].astype(str).str.split('.').str[0]\n",
    "    \n",
    "    # Apply the same for access_df\n",
    "    if 'CJ_Item' in access_df.columns:\n",
    "        access_df['CJ_Item'] = access_df['CJ_Item'].astype(str).str.split('.').str[0]\n",
    "    \n",
    "    # Apply the same for access_df2\n",
    "    if 'CJ_Item' in access_df2.columns:\n",
    "        access_df2['CJ_Item'] = access_df2['CJ_Item'].astype(str).str.split('.').str[0]\n",
    "    \n",
    "    return access_df, access_df2\n",
    "\n",
    "# Merge all DataFrames  \n",
    "def merge_dataframes(dfs, access_df):  \n",
    "    # Merge the 'CJ_Stock' DataFrame with 'Pivot SO' DataFrame  \n",
    "    merged_df = dfs['CJ_Stock'].merge(dfs['Daily_SO'],on='CJ_Item',how='outer',suffixes=('_from-CJ', '_from-DailySO'))  \n",
    "    \n",
    "    # Continue merging with other DataFrames  \n",
    "    merged_df = merged_df.merge(dfs['PO_HBA'], on='CJ_Item', how='outer', suffixes=('', '_from-HBA'))\n",
    "    merged_df = merged_df.merge(dfs['PO_Import'], on='CJ_Item', how='outer', suffixes=('_from-HBA', '_from-Import'))\n",
    "\n",
    "    # Rename SHM_Item for clarity  \n",
    "    dfs['Access_PO'] = dfs['Access_PO'].rename(columns={'SHM_Item': 'SHM_Item_from-All_PO'})  \n",
    "\n",
    "    # Merge with Access PO DataFrame  \n",
    "    merged_df = merged_df.merge(dfs['Access_PO'], on='CJ_Item', how='outer', suffixes=('_from-Import', '_from-All_PO'))  \n",
    "    \n",
    "    # Merge with Daily Stock DC DataFrame  \n",
    "    merged_df = merged_df.merge(dfs['Daily_Stock_DC'], on='CJ_Item', how='outer', suffixes=('_from-All_PO', '_from-DailyDC'))  \n",
    "\n",
    "    # Merge with Access database DataFrame  \n",
    "    merged_df = merged_df.merge(access_df, on='CJ_Item', how='left')\n",
    "\n",
    "    # Rename columns\n",
    "    merged_df = merged_df.rename(columns={  \n",
    "        'SHM_Item': 'SHM_Item_from_qry_Product_List',\n",
    "        'Division': 'Division_CJ_stock',  \n",
    "        'Devision': 'Division_SHM'  \n",
    "    })  \n",
    "\n",
    "    # Create a new column for NPD Status by First_SO_Date \n",
    "    today = pd.to_datetime(datetime.now().date())  \n",
    "    merged_df['days_from_first_ATP'] = (today - pd.to_datetime(merged_df['First_SO_Date'])).dt.days  \n",
    "    merged_df['NPD_Status'] = np.where(merged_df['days_from_first_ATP'] <= 31, 'NPD', '-')  \n",
    "\n",
    "    # Fill in missing values for descriptive columns  \n",
    "    merged_df['Name'] = merged_df['Name'].fillna(merged_df['Description'])  \n",
    "    merged_df['Category'] = merged_df['Category'].fillna(merged_df['Cat'])  \n",
    "    merged_df['Subcate'] = merged_df['Subcate'].fillna(merged_df['Sub_Cat'])  \n",
    "\n",
    "    # export merged_df to excel\n",
    "    merged_df.to_excel(r'D:\\Data for Stock Report\\debug.xlsx', index=False)\n",
    "    return merged_df\n",
    "\n",
    "# Create the 'SHM_Item' column based on priority from various sources\n",
    "def create_shm_item_column(merged_df):\n",
    "    merged_df['SHM_Item_from-All_PO'] = merged_df['SHM_Item_from-All_PO'].replace('', np.nan)\n",
    "    merged_df['SHM_Item_from-HBA'] = merged_df['SHM_Item_from-HBA'].replace('', np.nan)\n",
    "    merged_df['SHM_Item_from-Import'] = merged_df['SHM_Item_from-Import'].replace('', np.nan)\n",
    "    merged_df['SHM_Item_from_qry_Product_List'] = merged_df['SHM_Item_from_qry_Product_List'].replace('', np.nan)\n",
    "\n",
    "    merged_df['SHM_Item'] = np.where(\n",
    "        merged_df['SHM_Item_from-All_PO'].notna(), merged_df['SHM_Item_from-All_PO'],\n",
    "        np.where(merged_df['SHM_Item_from-HBA'].notna(), merged_df['SHM_Item_from-HBA'],\n",
    "                 np.where(merged_df['SHM_Item_from-Import'].notna(), merged_df['SHM_Item_from-Import'],\n",
    "                          merged_df['SHM_Item_from_qry_Product_List'].fillna('NO SHM Item in Access')))\n",
    "    )\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Fill NaN values in numeric columns with 0\n",
    "def fill_na_with_zero(df):\n",
    "    df[df.select_dtypes(include=[np.number]).columns] = df.select_dtypes(include=[np.number]).fillna(0)\n",
    "    return df\n",
    "\n",
    "def rearrange_columns(merged_df):\n",
    "    first_columns = ['CJ_Item','SHM_Item','SHM_Item_from-All_PO','SHM_Item_from-HBA','SHM_Item_from-Import','SHM_Item_from_qry_Product_List']\n",
    "    remaining_columns = [col for col in merged_df.columns if col not in first_columns]\n",
    "    final_column_order = first_columns + remaining_columns\n",
    "    merged_df = merged_df[final_column_order]\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Create new column to sum ALL PO Pending\n",
    "def calculate_totals(merged_df):\n",
    "    dc_columns = [1, 2, 4]\n",
    "    for dc in dc_columns:\n",
    "        merged_df[f'Total-PO_qty_to_DC{dc}'] = (\n",
    "            merged_df[f'PO_Qty_to_DC{dc}'] +\n",
    "            merged_df[f'PO_Qty_to_DC{dc}_from-Import'] + \n",
    "            merged_df[f'PO_Qty_to_DC{dc}_from-HBA'])\n",
    "\n",
    "        # Calculate %Ratio with error handling\n",
    "        merged_df[f'%Ratio_AvgSalesQty90D_DC{dc}'] = (\n",
    "            merged_df[f'DC{dc}_AvgSaleQty90D'] / merged_df['Total_AvgSaleQty90D'].replace(0, 1)\n",
    "        ).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    # Calculate Total Remain Stock\n",
    "    merged_df['Remain_StockQty_AllDC'] = merged_df['DC1_Remain_StockQty'] + merged_df['DC2_Remain_StockQty'] + merged_df['DC4_Remain_StockQty']\n",
    "    merged_df['Remain_StockValue_AllDC'] = merged_df['DC1_Remain_StockValue'] + merged_df['DC2_Remain_StockValue'] + merged_df['DC4_Remain_StockValue']\n",
    "\n",
    "    # Calculate SO Qty\n",
    "    for dc in dc_columns:\n",
    "        merged_df[f'DC{dc}_SO_Last30D'] = round(merged_df['SO_Qty_last30D'] * merged_df[f'%Ratio_AvgSalesQty90D_DC{dc}'])\n",
    "        merged_df[f'DC{dc}_SO_Last7D'] = round(merged_df['SO_Qty_last7D'] * merged_df[f'%Ratio_AvgSalesQty90D_DC{dc}'])\n",
    "\n",
    "        merged_df[f'DC{dc}_AvgSaleQty30D'] = merged_df[f'DC{dc}_SO_Last30D'] / 30\n",
    "        merged_df[f'DC{dc}_AvgSaleQty7D'] = merged_df[f'DC{dc}_SO_Last7D'] / 7\n",
    "\n",
    "    merged_df['Total_AvgSaleQty30D'] = merged_df[[f'DC{dc}_AvgSaleQty30D' for dc in dc_columns]].sum(axis=1)\n",
    "    merged_df['Total_AvgSaleQty7D'] = merged_df[[f'DC{dc}_AvgSaleQty7D' for dc in dc_columns]].sum(axis=1)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Simplify DOH calculation\n",
    "def calculate_DOH(stock_value, avg_cogs):\n",
    "    return np.where(\n",
    "        (stock_value != 0) & (avg_cogs == 0), np.inf,\n",
    "        np.where((stock_value == 0) & (avg_cogs == 0), 0, stock_value / avg_cogs)\n",
    "    )\n",
    "\n",
    "# Simplified DOH calculations for various stock locations\n",
    "def apply_doh_calculations(merged_df):\n",
    "    merged_df['Current_DOH_All_DC'] = calculate_DOH(merged_df['Remain_StockQty_AllDC'], merged_df['Total_AvgSaleQty90D'])\n",
    "    merged_df['Current_DC1_DOH'] = calculate_DOH(merged_df['DC1_Remain_StockQty'], merged_df['DC1_AvgSaleQty90D'])\n",
    "    merged_df['Current_DC2_DOH'] = calculate_DOH(merged_df['DC2_Remain_StockQty'], merged_df['DC2_AvgSaleQty90D'])\n",
    "    merged_df['Current_DC4_DOH'] = calculate_DOH(merged_df['DC4_Remain_StockQty'], merged_df['DC4_AvgSaleQty90D'])\n",
    "\n",
    "    # Create new col for calculate\n",
    "    merged_df['Total-PO_qty_to_DC'] = merged_df['Total-PO_qty_to_DC1'] + merged_df['Total-PO_qty_to_DC2'] + merged_df['Total-PO_qty_to_DC4']\n",
    "    merged_df['Current_DOH(Stock+PO)_All_DC'] = calculate_DOH(merged_df['Remain_StockQty_AllDC'] + merged_df['Total-PO_qty_to_DC'], merged_df['Total_AvgSaleQty90D'])\n",
    "    merged_df['DC1_DOH(Stock+PO)'] = calculate_DOH(merged_df['DC1_Remain_StockQty'] + merged_df['Total-PO_qty_to_DC1'], merged_df['DC1_AvgSaleQty90D'])\n",
    "    merged_df['DC2_DOH(Stock+PO)'] = calculate_DOH(merged_df['DC2_Remain_StockQty'] + merged_df['Total-PO_qty_to_DC2'], merged_df['DC2_AvgSaleQty90D'])\n",
    "    merged_df['DC4_DOH(Stock+PO)'] = calculate_DOH(merged_df['DC4_Remain_StockQty'] + merged_df['Total-PO_qty_to_DC4'], merged_df['DC4_AvgSaleQty90D'])\n",
    "\n",
    "\n",
    "    merged_df['Min_delivery_date_to_DC1'] = merged_df[['Min_del_date_to_DC1', 'Min_del_date_to_DC1_from-Import', 'Min_del_date_to_DC1_from-HBA']].min(axis=1)\n",
    "    merged_df['Min_delivery_date_to_DC2'] = merged_df[['Min_del_date_to_DC2', 'Min_del_date_to_DC2_from-Import', 'Min_del_date_to_DC2_from-HBA']].min(axis=1)\n",
    "    merged_df['Min_delivery_date_to_DC4'] = merged_df[['Min_del_date_to_DC4','Min_del_date_to_DC4_from-Import','Min_del_date_to_DC4_from-HBA']].min(axis=1)\n",
    "    merged_df['Min_delivery_date_to_DC'] = merged_df[['Min_delivery_date_to_DC1', 'Min_delivery_date_to_DC2','Min_delivery_date_to_DC4']].min(axis=1)\n",
    "\n",
    "    # Calculate Stock cover date as a date time format\n",
    "    current_date = pd.to_datetime(datetime.now().date())\n",
    "    # Define maximum value for calculation\n",
    "    max_doh_value = 1825\n",
    "\n",
    "    # Set Current DOH if exceeding the max DOH, then = infinite\n",
    "    merged_df['Current_DOH_All_DC'] = np.where(merged_df['Current_DOH_All_DC'] > max_doh_value, np.inf, merged_df['Current_DOH_All_DC'])\n",
    "    merged_df['Current_DC1_DOH'] = np.where(merged_df['Current_DC1_DOH'] > max_doh_value, np.inf, merged_df['Current_DC1_DOH'])\n",
    "    merged_df['Current_DC2_DOH'] = np.where(merged_df['Current_DC2_DOH'] > max_doh_value, np.inf, merged_df['Current_DC2_DOH'])\n",
    "    merged_df['Current_DC4_DOH'] = np.where(merged_df['Current_DC4_DOH'] > max_doh_value, np.inf, merged_df['Current_DC4_DOH'])\n",
    "\n",
    "    # Also set DOH of Stock+PO to inf\n",
    "    merged_df['Current_DOH(Stock+PO)_All_DC'] = np.where(merged_df['Current_DOH(Stock+PO)_All_DC'] > max_doh_value, np.inf, merged_df['Current_DOH(Stock+PO)_All_DC'])\n",
    "    merged_df['DC1_DOH(Stock+PO)'] = np.where(merged_df['DC1_DOH(Stock+PO)'] > max_doh_value, np.inf, merged_df['DC1_DOH(Stock+PO)'])\n",
    "    merged_df['DC2_DOH(Stock+PO)'] = np.where(merged_df['DC2_DOH(Stock+PO)'] > max_doh_value, np.inf, merged_df['DC2_DOH(Stock+PO)'])\n",
    "    merged_df['DC4_DOH(Stock+PO)'] = np.where(merged_df['DC4_DOH(Stock+PO)'] > max_doh_value, np.inf, merged_df['DC4_DOH(Stock+PO)'])\n",
    "\n",
    "    # Create new columns and initialize with NaT\n",
    "    merged_df['Total_Store_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock_All_DC_Cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock+PO_All_DC_Cover_to_date'] = pd.NaT\n",
    "    merged_df['Store_DC1_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock_DC1_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock+PO_DC1_cover_to_date'] = pd.NaT\n",
    "    merged_df['Store_DC2_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock_DC2_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock+PO_DC2_cover_to_date'] = pd.NaT\n",
    "    merged_df['Store_DC4_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock_DC4_cover_to_date'] = pd.NaT\n",
    "    merged_df['Stock+PO_DC4_cover_to_date'] = pd.NaT\n",
    "\n",
    "\n",
    "    # Loop through each row to handle calculations\n",
    "    for index, row in merged_df.iterrows():\n",
    "        # Process Total Stores DOH\n",
    "        all_store_doh = row['Total_DOHStore']\n",
    "        if pd.notnull(all_store_doh) and all_store_doh > 0 and not np.isinf(all_store_doh) and all_store_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Total_Store_cover_to_date'] = current_date + pd.to_timedelta(all_store_doh, unit='d')\n",
    "\n",
    "        # Process for Current DOH All DC\n",
    "        Current_DOH_All_DC = row['Current_DOH_All_DC']\n",
    "        if pd.notnull(Current_DOH_All_DC) and Current_DOH_All_DC > 0 and not np.isinf(Current_DOH_All_DC) and Current_DOH_All_DC <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock_All_DC_Cover_to_date'] = current_date + pd.to_timedelta(Current_DOH_All_DC, unit='d')\n",
    "\n",
    "        # Process for Current Stock + PO\n",
    "        doh_all_dc_plus_po = row['Current_DOH(Stock+PO)_All_DC']\n",
    "        if pd.notnull(doh_all_dc_plus_po) and doh_all_dc_plus_po > 0 and not np.isinf(doh_all_dc_plus_po) and doh_all_dc_plus_po <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock+PO_All_DC_Cover_to_date'] = current_date + pd.to_timedelta(doh_all_dc_plus_po, unit='d')\n",
    "\n",
    "        # Process Stores DC1 DOH\n",
    "        DC1_store_doh = row['DC1_DOHStore']\n",
    "        if pd.notnull(DC1_store_doh) and DC1_store_doh > 0 and not np.isinf(DC1_store_doh) and DC1_store_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Store_DC1_cover_to_date'] = current_date + pd.to_timedelta(DC1_store_doh, unit='d')\n",
    "\n",
    "        # Process for Current DC1 DOH\n",
    "        current_dc1_doh = row['Current_DC1_DOH']\n",
    "        if pd.notnull(current_dc1_doh) and current_dc1_doh > 0 and not np.isinf(current_dc1_doh) and current_dc1_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock_DC1_cover_to_date'] = current_date + pd.to_timedelta(current_dc1_doh, unit='d')\n",
    "\n",
    "        # Process for Current Stock + PO DC1\n",
    "        doh_dc1_plus_po = row['DC1_DOH(Stock+PO)']\n",
    "        if pd.notnull(doh_dc1_plus_po) and doh_dc1_plus_po > 0 and not np.isinf(doh_dc1_plus_po) and doh_dc1_plus_po <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock+PO_DC1_cover_to_date'] = current_date + pd.to_timedelta(doh_dc1_plus_po, unit='d')\n",
    "\n",
    "        # Process Stores DC2 DOH\n",
    "        DC2_store_doh = row['DC2_DOHStore']\n",
    "        if pd.notnull(DC2_store_doh) and DC2_store_doh > 0 and not np.isinf(DC2_store_doh) and DC2_store_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Store_DC2_cover_to_date'] = current_date + pd.to_timedelta(DC2_store_doh, unit='d')\n",
    "\n",
    "        # Process for Current DC2 DOH\n",
    "        current_dc2_doh = row['Current_DC2_DOH']\n",
    "        if pd.notnull(current_dc2_doh) and current_dc2_doh > 0 and not np.isinf(current_dc2_doh) and current_dc2_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock_DC2_cover_to_date'] = current_date + pd.to_timedelta(current_dc2_doh, unit='d')\n",
    "\n",
    "        # Process for Current Stock + PO DC2\n",
    "        doh_dc2_plus_po = row['DC2_DOH(Stock+PO)']\n",
    "        if pd.notnull(doh_dc2_plus_po) and doh_dc2_plus_po > 0 and not np.isinf(doh_dc2_plus_po) and doh_dc2_plus_po <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock+PO_DC2_cover_to_date'] = current_date + pd.to_timedelta(doh_dc2_plus_po, unit='d')\n",
    "\n",
    "        # Process Stores DC4 DOH\n",
    "        DC4_store_doh = row['DC4_DOHStore']\n",
    "        if pd.notnull(DC4_store_doh) and DC4_store_doh > 0 and not np.isinf(DC4_store_doh) and DC4_store_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Store_DC4_cover_to_date'] = current_date + pd.to_timedelta(DC4_store_doh, unit='d')            \n",
    "\n",
    "        # Process for Current DC4 DOH\n",
    "        current_dc4_doh = row['Current_DC4_DOH']\n",
    "        if pd.notnull(current_dc4_doh) and current_dc4_doh > 0 and not np.isinf(current_dc4_doh) and current_dc4_doh <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock_DC4_cover_to_date'] = current_date + pd.to_timedelta(current_dc4_doh, unit='d')\n",
    "\n",
    "        # Process for Current Stock + PO DC4\n",
    "        doh_dc4_plus_po = row['DC4_DOH(Stock+PO)']\n",
    "        if pd.notnull(doh_dc4_plus_po) and doh_dc4_plus_po > 0 and not np.isinf(doh_dc4_plus_po) and doh_dc4_plus_po <= max_doh_value:\n",
    "            merged_df.at[index, 'Stock+PO_DC4_cover_to_date'] = current_date + pd.to_timedelta(doh_dc4_plus_po, unit='d')\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    dataframes = load_data(file_paths)\n",
    "    access_df, access_df2 = load_access_data()\n",
    "    access_df, access_df2 = convert_cj_item_to_string(dataframes, access_df, access_df2)\n",
    "    \n",
    "    merged_df = merge_dataframes(dataframes, access_df)\n",
    "    merged_df = create_shm_item_column(merged_df)\n",
    "\n",
    "    # Merge with access_df again to get the correct supplier name\n",
    "    merged_df = pd.merge(merged_df, access_df2, on=['CJ_Item', 'SHM_Item'], how='left')\n",
    "    # Merge with master Owner and lead time to get owner name and lead time\n",
    "    merged_df = merged_df.merge(dataframes['Master_Owner&LT'],on=['SHM_Item','CJ_Item'],how='left')\n",
    "    merged_df = merged_df.fillna({'OwnerSCM': 'No data', 'LeadTime': 'No data'})\n",
    "\n",
    "    merged_df = fill_na_with_zero(merged_df)\n",
    "    merged_df = rearrange_columns(merged_df)\n",
    "    merged_df = calculate_totals(merged_df)\n",
    "    merged_df = apply_doh_calculations(merged_df)\n",
    "\n",
    "   # Execute SQL query\n",
    "    query = \"\"\"\n",
    "    SELECT Division_SHM,\n",
    "        OwnerSCM,\n",
    "        [Supplier Name],\n",
    "        SHM_Item,\n",
    "        CJ_Item,\n",
    "        Name,\n",
    "        Category,\n",
    "        Brand,\n",
    "        LeadTime,\n",
    "        Status,\n",
    "        [Group],\n",
    "        Unit,\n",
    "        PC_Cartons,\n",
    "        First_SO_Date,\n",
    "        NPD_Status,\n",
    "        Total_ScmAssort,\n",
    "        Total_OOSAssort,\n",
    "        Total_CountOKROOS,\n",
    "        Total_PercOOS,\n",
    "        Total_StoreStockQty,\n",
    "        Total_DOHStore,\n",
    "        Total_Store_cover_to_date,\n",
    "        Total_AvgSaleQty90D,\n",
    "        Total_AvgSaleQty30D,\n",
    "        Total_AvgSaleQty7D,\n",
    "        SO_Qty_last7D,\n",
    "        Remain_StockQty_AllDC,\n",
    "        Current_DOH_All_DC,\n",
    "        Stock_All_DC_Cover_to_date,\n",
    "        [Total-PO_qty_to_DC],\n",
    "        Min_delivery_date_to_DC,\n",
    "        [Current_DOH(Stock+PO)_All_DC],\n",
    "        [Stock+PO_All_DC_Cover_to_date],\n",
    "        DC1_ScmAssort,\n",
    "        DC1_OOSAssort,\n",
    "        DC1_CountOKROOS,\n",
    "        DC1_PercOOS,\n",
    "        DC1_StoreStockQty,\n",
    "        DC1_DOHStore,\n",
    "        Store_DC1_cover_to_date,\n",
    "        DC1_AvgSaleQty90D,\n",
    "        DC1_AvgSaleQty30D,\n",
    "        DC1_AvgSaleQty7D,\n",
    "        [%Ratio_AvgSalesQty90D_DC1],\n",
    "        DC1_Remain_StockQty,\n",
    "        Current_DC1_DOH,\n",
    "        Stock_DC1_cover_to_date,\n",
    "        [Total-PO_qty_to_DC1],\n",
    "        Min_delivery_date_to_DC1,\n",
    "        [DC1_DOH(Stock+PO)],\n",
    "        [Stock+PO_DC1_cover_to_date],\n",
    "        DC2_ScmAssort,\n",
    "        DC2_OOSAssort,\n",
    "        DC2_CountOKROOS,\n",
    "        DC2_PercOOS,\n",
    "        DC2_StoreStockQty,\n",
    "        DC2_DOHStore,\n",
    "        Store_DC2_cover_to_date,\n",
    "        DC2_AvgSaleQty90D,\n",
    "        DC2_AvgSaleQty30D,\n",
    "        DC2_AvgSaleQty7D,\n",
    "        [%Ratio_AvgSalesQty90D_DC2],\n",
    "        DC2_Remain_StockQty,\n",
    "        Current_DC2_DOH,\n",
    "        Stock_DC2_cover_to_date,\n",
    "        [Total-PO_qty_to_DC2],\n",
    "        Min_delivery_date_to_DC2,\n",
    "        [DC2_DOH(Stock+PO)],\n",
    "        [Stock+PO_DC2_cover_to_date],\n",
    "        DC4_ScmAssort,\n",
    "        DC4_OOSAssort,\n",
    "        DC4_CountOKROOS,\n",
    "        DC4_PercOOS,\n",
    "        DC4_StoreStockQty,\n",
    "        DC4_DOHStore,\n",
    "        Store_DC4_cover_to_date,\n",
    "        DC4_AvgSaleQty90D,\n",
    "        DC4_AvgSaleQty30D,\n",
    "        DC4_AvgSaleQty7D,\n",
    "        [%Ratio_AvgSalesQty90D_DC4],\n",
    "        DC4_Remain_StockQty,\n",
    "        Current_DC4_DOH,\n",
    "        Stock_DC4_cover_to_date,\n",
    "        [Total-PO_qty_to_DC4],\n",
    "        Min_delivery_date_to_DC4,\n",
    "        [DC4_DOH(Stock+PO)],\n",
    "        [Stock+PO_DC4_cover_to_date]\n",
    "    FROM merged_df\n",
    "    WHERE ([Group] != 'Discontinuous' or [Group] IS NULL)\n",
    "    \"\"\"\n",
    "\n",
    "    result_df = ps.sqldf(query, locals())\n",
    "    \n",
    "\n",
    "    # Rename All column for store to display a data this column is freezed\n",
    "    columns_to_rename = {\n",
    "        'Status': 'CJ_Status',\n",
    "        'Group': 'SHM_Status',\n",
    "        'Unit': 'Unit_of_Purchase',\n",
    "        'LeadTime':'LeadTime(Days)',\n",
    "        'Total_OOSAssort':'Total_ActiveAssort',\n",
    "        'Total_CountOKROOS':'Total_StoreOOS',\n",
    "        'Total_PercOOS':'Total_%StoreOOS'\n",
    "        }\n",
    "    \n",
    "    # Rename the columns\n",
    "    result_df.rename(columns=columns_to_rename, inplace=True)\n",
    "    renamed_columns = [columns_to_rename[old_name] for old_name in columns_to_rename if old_name in result_df.columns]\n",
    "    print(f\"The following columns have been renamed: {columns_to_rename}\")\n",
    "\n",
    "\n",
    "    # Filter to show the data only all numeric column > 0\n",
    "    numeric_cols = result_df.select_dtypes(include=[np.number]).columns\n",
    "    result_df = result_df[(result_df[numeric_cols] != 0).any(axis=1)]\n",
    "\n",
    "    # Format the date columns to display in Excel\n",
    "    date_columns = [\n",
    "        'First_SO_Date',\n",
    "        'Min_delivery_date_to_DC',\n",
    "        'Min_delivery_date_to_DC1',\n",
    "        'Min_delivery_date_to_DC2',\n",
    "        'Min_delivery_date_to_DC4',\n",
    "        'Total_Store_cover_to_date',\n",
    "        'Store_DC1_cover_to_date',\n",
    "        'Store_DC2_cover_to_date',\n",
    "        'Store_DC4_cover_to_date',\n",
    "        'Stock_All_DC_Cover_to_date',\n",
    "        'Stock_DC1_cover_to_date',\n",
    "        'Stock_DC2_cover_to_date',\n",
    "        'Stock_DC4_cover_to_date',\n",
    "        'Stock+PO_All_DC_Cover_to_date',\n",
    "        'Stock+PO_DC1_cover_to_date',\n",
    "        'Stock+PO_DC2_cover_to_date',\n",
    "        'Stock+PO_DC4_cover_to_date'\n",
    "    ]\n",
    "\n",
    "    for col in date_columns:\n",
    "        if col in result_df.columns:\n",
    "            result_df[col] = pd.to_datetime(result_df[col], errors='coerce')  # Ensure it's in datetime format\n",
    "    \n",
    "\n",
    "    # Drop Duplicate rows when all data in each columns is duplicated except in the []\n",
    "    column_to_check = result_df.columns.difference(['SHM_Item','SHM_Status'])  # get all column except in []\n",
    "    deduplicated_df = result_df.drop_duplicates(subset=column_to_check)\n",
    "\n",
    "\n",
    "    # Drop SHM_Item = No shm item in access and Save deduplicated_df to excel\n",
    "    #deduplicated_df = deduplicated_df[deduplicated_df['SHM_Item'] != 'NO SHM Item in Access']\n",
    "    #deduplicated_df.to_excel(r'D:\\Data for Stock Report\\Data_for_OOS_Log\\3.Mar_10-03-2025.xlsx', index=False)\n",
    "\n",
    "    # Handle duplicates 'CJ_Item' by using groupby\n",
    "    def replace_cj_duplicate(group):\n",
    "        # For each numeric check for duplicates\n",
    "        for col in numeric_cols:\n",
    "            if col == 'PC_Cartons':\n",
    "                continue  # Skip when column = pc_carton\n",
    "            if group[col].nunique() == 1:  # All values are the same in this group\n",
    "                group.iloc[1:, group.columns.get_loc(col)] = 0  # Replace all but keep the first row\n",
    "        \n",
    "        # Filter these 2 columns if both columns are 0 then drop the row, keep the first row\n",
    "        group = group[(group[['Remain_StockQty_AllDC', 'Total-PO_qty_to_DC']].ne(0).any(axis=1)) | (group.index == group.index.min())]\n",
    "\n",
    "        return group\n",
    "    \n",
    "    # Apply this function to each group of 'CJ_Item'\n",
    "    deduplicated_df = deduplicated_df.groupby('CJ_Item').apply(replace_cj_duplicate).reset_index(drop=True)\n",
    "\n",
    "    # Rename column for converting QTY to BOX QTY\n",
    "    column_rename_mapping = {\n",
    "        'Total_AvgSaleQty90D': 'Total_AvgSaleCTN_Last90D',\n",
    "        'Total_AvgSaleQty30D': 'Total_AvgSaleCTN_Last30D',\n",
    "        'Total_AvgSaleQty7D': 'Total_AvgSaleCTN_Last7D',\n",
    "        'DC1_AvgSaleQty90D': 'DC1_AvgSaleCTN_Last90Days',\n",
    "        'DC1_AvgSaleQty30D': 'DC1_AvgSaleCTN_Last30Days',\n",
    "        'DC1_AvgSaleQty7D': 'DC1_AvgSaleCTN_Last7Days',\n",
    "        'DC2_AvgSaleQty90D': 'DC2_AvgSaleCTN_Last90Days',\n",
    "        'DC2_AvgSaleQty30D': 'DC2_AvgSaleCTN_Last30Days',\n",
    "        'DC2_AvgSaleQty7D': 'DC2_AvgSaleCTN_Last7Days',\n",
    "        'DC4_AvgSaleQty90D': 'DC4_AvgSaleCTN_Last90Days',\n",
    "        'DC4_AvgSaleQty30D': 'DC4_AvgSaleCTN_Last30Days',\n",
    "        'DC4_AvgSaleQty7D': 'DC4_AvgSaleCTN_Last7Days',\n",
    "        'Total_StoreStockQty': 'Total_StoreStockCTN',\n",
    "        'DC1_StoreStockQty': 'DC1_StoreStockCTN',\n",
    "        'DC2_StoreStockQty': 'DC2_StoreStockCTN',\n",
    "        'DC4_StoreStockQty': 'DC4_StoreStockCTN',\n",
    "        'SO_Qty_last7D': 'SO_CTN_last7D',\n",
    "        'Remain_StockQty_AllDC': 'Remain_CTN_AllDC',\n",
    "        'DC1_Remain_StockQty': 'DC1_Remain_CTN',\n",
    "        'DC2_Remain_StockQty': 'DC2_Remain_CTN',\n",
    "        'DC4_Remain_StockQty': 'DC4_Remain_CTN',\n",
    "        'Total-PO_qty_to_DC': 'Total-CTN_to_DC',\n",
    "        'Total-PO_qty_to_DC1': 'Total-CTN_to_DC1',\n",
    "        'Total-PO_qty_to_DC2': 'Total-CTN_to_DC2',\n",
    "        'Total-PO_qty_to_DC4': 'Total-CTN_to_DC4'\n",
    "    }\n",
    "\n",
    "    # Process the rename\n",
    "    modified_df = deduplicated_df.rename(columns=column_rename_mapping)\n",
    "\n",
    "    # List of new columns that were just renamed\n",
    "    renamed_columns = list(column_rename_mapping.values())\n",
    "    \n",
    "    # Loop through renamed column to perform division by PC_Cartons\n",
    "    for col in renamed_columns:\n",
    "        modified_df[col] = np.where(\n",
    "            modified_df['PC_Cartons'] == 0,0,\n",
    "            modified_df[col] / modified_df['PC_Cartons']\n",
    "        )\n",
    "\n",
    "    # Define current date and save path \n",
    "    current_date = datetime.now().strftime('%d-%m-%Y')\n",
    "    save_directory = r'D:\\Data for Stock Report\\Completed Daily Stock Report'\n",
    "    file_name = f\"Sahamit Daily Stock Report_{current_date}.xlsx\"\n",
    "    save_path = os.path.join(save_directory, file_name)\n",
    "\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    # Save the merged DataFrame\n",
    "    try: \n",
    "        with pd.ExcelWriter(save_path, mode='w') as writer:\n",
    "            deduplicated_df.to_excel(writer, sheet_name='Data by Qty', index=False)\n",
    "            modified_df.to_excel(writer, sheet_name='Data by Cartons',index=False)\n",
    "\n",
    "            # For checking Raw data\n",
    "            #merged_df.to_excel(writer, sheet_name='Raw Data', index=False)\n",
    "            #result_df.to_excel(writer, sheet_name='after query', index=False)\n",
    "\n",
    "            # Load All sheet from Raw_PO_pending and save them together\n",
    "            existing_file_path = r'D:\\Data for Stock Report\\Raw_PO_pending.xlsx'\n",
    "            if os.path.exists(existing_file_path):\n",
    "                workbook = load_workbook(existing_file_path)\n",
    "                for sheet_name in workbook.sheetnames:\n",
    "                    existing_df = pd.read_excel(existing_file_path,sheet_name=sheet_name)\n",
    "                    existing_df.to_excel(writer,sheet_name=sheet_name,index=False)\n",
    "            else:\n",
    "                print(f\"File not found: {existing_file_path}\")\n",
    "\n",
    "        print(f\"Data merged and saved successfully at {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
