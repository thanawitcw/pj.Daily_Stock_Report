{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  owner_scm_df_selected.rename(columns={'Base Lead Time (Days)': 'LeadTime'}, inplace=True)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].astype(str)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].fillna('')\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].apply(lambda x: x.split('.')[0] if '.0' in str(x) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:101: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  access_df = pd.read_sql(query, conn)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:102: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  query2_df = pd.read_sql(query2, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to Access database is closed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:247: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[f'Min_delivery_date_to_{dc}'] = merged_df[cols].min(axis=1)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:247: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[f'Min_delivery_date_to_{dc}'] = merged_df[cols].min(axis=1)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:247: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[f'Min_delivery_date_to_{dc}'] = merged_df[cols].min(axis=1)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:249: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df['Min_delivery_date_to_DC'] = merged_df[[f'Min_delivery_date_to_{dc}' for dc in dc_list]].min(axis=1)\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[col] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[col] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[col] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[col] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[col] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[col] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[col] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[col] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[col] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[col] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[col] = pd.NaT\n",
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged_df[col] = pd.NaT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following columns have been renamed: {'Status': 'CJ_Status', 'Group': 'SHM_Status', 'Unit': 'Unit_of_Purchase', 'LeadTime': 'LeadTime(Days)', 'Total_OOSAssort': 'Total_ActiveAssort', 'Total_CountOKROOS': 'Total_StoreOOS', 'Total_PercOOS': 'Total_%StoreOOS'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thanawit C\\AppData\\Local\\Temp\\ipykernel_15256\\1494688372.py:491: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result_df = result_df.groupby('CJ_Item', group_keys=False).apply(replace_cj_duplicate).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed daily stock report has been saved to D:\\Data for Stock Report\\Completed Daily Stock Report\\Sahamit Daily Stock Report_24-11-2025.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import os  \n",
    "import pyodbc  \n",
    "import pandasql as ps  \n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Paths to all cleaned stock files  \n",
    "file_paths = {  \n",
    "    'CJ_Stock': r'D:\\Data for Stock Report\\cleaned_CJ_Stock_Report.xlsx',  \n",
    "    'Daily_SO': r'D:\\Data for Stock Report\\appended_cleaned_SellOut.xlsx',\n",
    "    'Access_PO': r'D:\\Data for Stock Report\\cleaned_PO_pending_other.xlsx',  \n",
    "    'Daily_Stock_DC': r'D:\\Data for Stock Report\\cleaned_DC_daily_stock.xlsx',\n",
    "    'Master_Owner&LT': r'C:\\Users\\Thanawit C\\OneDrive - Sahamit Product Co.,Ltd\\Data for Stock Report\\COPY_MasterLeadTime.xlsx'  \n",
    "}  \n",
    "\n",
    "# Load Excel files into DataFrames\n",
    "def load_data(file_paths):\n",
    "    dataframes = {}\n",
    "    for name, path in file_paths.items():\n",
    "        try:\n",
    "            if name == \"Master_Owner&LT\":\n",
    "                # Load the specified sheet\n",
    "                owner_scm_df = pd.read_excel(path, sheet_name='All_Product', header=1) \n",
    "                \n",
    "                # Select specific columns\n",
    "                owner_scm_df_selected = owner_scm_df[['SHM_Item', 'CJ_Item', 'OwnerSCM', 'Base Lead Time (Days)']]\n",
    "                owner_scm_df_selected.rename(columns={'Base Lead Time (Days)': 'LeadTime'}, inplace=True)\n",
    "\n",
    "                # Cast CJ_Item to str\n",
    "                owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].astype(str)\n",
    "                # Clean CJ_Item column\n",
    "                owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].fillna('')\n",
    "                owner_scm_df_selected['CJ_Item'] = owner_scm_df_selected['CJ_Item'].apply(lambda x: x.split('.')[0] if '.0' in str(x) else x)\n",
    "\n",
    "                # Store the processed DataFrame\n",
    "                dataframes[name] = owner_scm_df_selected\n",
    "            else:\n",
    "                # Load other sheets  \n",
    "                sheet_name = {\n",
    "                    'CJ_Stock': 'CJ Stock',\n",
    "                    'Daily_SO': 'Pivot SO',\n",
    "                    'Access_PO': 'Pivot All PO pending',  \n",
    "                    'Daily_Stock_DC': 'Pivot_DC_stock'\n",
    "                }.get(name)  \n",
    "\n",
    "                dataframes[name] = pd.read_excel(path, sheet_name=sheet_name, parse_dates=False)\n",
    "        except Exception as e:  \n",
    "            print(f\"Error loading {name}: {e}\")\n",
    "    return dataframes\n",
    "\n",
    "# Connect to Access database and fetch product details  \n",
    "def load_access_data():  \n",
    "    access_db_path = r'D:\\DataBase Access\\SHM_TMS_001_Master_Copy.accdb'  \n",
    "    conn_str = (  \n",
    "        r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'  \n",
    "        f'DBQ={access_db_path};'  \n",
    "    )  \n",
    "    try:  \n",
    "        conn = pyodbc.connect(conn_str)  \n",
    "        print(\"Connection successful\")  \n",
    "\n",
    "        query = \"\"\"  \n",
    "        SELECT t1.CJ_Item,\n",
    "               t1.SHM_Item,  \n",
    "               t1.CJ_Description,  \n",
    "               t1.Devision,  \n",
    "               t1.[Group],  \n",
    "               t1.PC_Cartons,  \n",
    "               t1.CJ_Status,  \n",
    "               t1.Cat,  \n",
    "               t1.Sub_Cat,  \n",
    "               t1.Brand,\n",
    "               First_SO_Date,\n",
    "               t2.Supplier_Code,\n",
    "               t2.[Supplier Name]\n",
    "          FROM tbl_master_003_product_list AS t1\n",
    "          LEFT JOIN qry_Product_list AS t2\n",
    "          ON t1.SHM_Item = t2.SHM_Item\n",
    "        \"\"\"\n",
    "\n",
    "        query2 = \"\"\"  \n",
    "        SELECT \n",
    "            t2.Item,\n",
    "            MIN(t2.Unit) AS Unit\n",
    "        FROM (\n",
    "            SELECT Item, Unit, [PO Date] AS po_date\n",
    "            FROM tbl_AllPO_Details\n",
    "            ) AS t2\n",
    "        INNER JOIN (\n",
    "            SELECT Item, MAX([PO Date]) AS max_date\n",
    "            FROM tbl_AllPO_Details\n",
    "            Group by Item\n",
    "            ) AS t1\n",
    "        ON t1.Item = t2.item AND t2.po_date = t1.max_date\n",
    "        GROUP BY t2.Item\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute the queries\n",
    "        access_df = pd.read_sql(query, conn)\n",
    "        query2_df = pd.read_sql(query2, conn)\n",
    "\n",
    "        # merge for the unit of purchase product info\n",
    "        access_df2 = pd.merge(access_df, query2_df, left_on='SHM_Item', right_on='Item', how='left')\n",
    "\n",
    "        conn.close()  \n",
    "        print(\"Connection to Access database is closed.\")  \n",
    "        return access_df2\n",
    "    \n",
    "    except Exception as e:  \n",
    "        print(f\"Connection failed: {e}\")  \n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "\n",
    "# Convert 'CJ_Item' to string format in all DataFrames  \n",
    "def convert_cj_item_to_string(dataframes, access_df2):\n",
    "    def process_column(df, column):\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].astype(str).str.split('.').str[0]\n",
    "\n",
    "    for df in dataframes.values():\n",
    "        process_column(df, 'CJ_Item')\n",
    "        process_column(df, 'SHM_Item')\n",
    "\n",
    "    # Cast CJ_Item to str (when the access_df2 is a dataframe not dictionary)\n",
    "    process_column(access_df2, 'CJ_Item')\n",
    "    process_column(access_df2, 'SHM_Item') \n",
    "\n",
    "    return access_df2\n",
    "\n",
    "# Merge all DataFrames  \n",
    "def merge_dataframes(dfs, access_df):\n",
    "    merged_df = access_df.merge(dfs['CJ_Stock'], on='CJ_Item', how='left')\n",
    "\n",
    "    merged_df = merged_df.merge(dfs['Daily_SO'], on='CJ_Item', how='left')  \n",
    "\n",
    "    merged_df = merged_df.merge(dfs['Access_PO'], on='SHM_Item', how='outer', suffixes =('', '_from-AccessPO'))\n",
    "\n",
    "    merged_df = merged_df.merge(dfs['Daily_Stock_DC'], on='CJ_Item', how='left', suffixes=('', '_from-DailyDC'))  \n",
    "\n",
    "    merged_df = merged_df.rename(columns={  \n",
    "        'Division': 'Division_CJ_stock',  \n",
    "        'Devision': 'Division_SHM'  \n",
    "    })  \n",
    "\n",
    "    # Create a new column for NPD Status by First_SO_Date, current logic = First SO date + 15 days\n",
    "    today = pd.to_datetime(datetime.now().date())  \n",
    "    merged_df['days_from_first_ATP'] = (today - pd.to_datetime(merged_df['First_SO_Date'])).dt.days  \n",
    "    merged_df['NPD_Status'] = np.where(merged_df['days_from_first_ATP'] <= 15, 'NPD', '-')\n",
    "\n",
    "    # Fill in missing values for descriptive columns  \n",
    "    merged_df['Name'] = merged_df['Name'].fillna(merged_df['CJ_Description'])  \n",
    "    merged_df['Category'] = merged_df['Category'].fillna(merged_df['Cat'])  \n",
    "    merged_df['Subcate'] = merged_df['Subcate'].fillna(merged_df['Sub_Cat'])  \n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Fill NaN values in numeric columns with 0\n",
    "def fill_na_with_zero(df):\n",
    "    df[df.select_dtypes(include=[np.number]).columns] = df.select_dtypes(include=[np.number]).fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Create new column to sum ALL PO Pending\n",
    "def calculate_totals(merged_df):\n",
    "    dc_columns = [1, 2, 4]\n",
    "    for dc in dc_columns:\n",
    "        merged_df[f'Total-PO_qty_to_DC{dc}'] = (\n",
    "            merged_df[f'PO_Qty_to_DC{dc}'])\n",
    "\n",
    "        # Calculate %Ratio with error handling\n",
    "        merged_df[f'%Ratio_AvgSalesQty90D_DC{dc}'] = (\n",
    "            merged_df[f'DC{dc}_AvgSaleQty90D'] / merged_df['Total_AvgSaleQty90D'].replace(0, 1)\n",
    "        ).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    # Calculate Total Remain Stock\n",
    "    merged_df['Remain_StockQty_AllDC'] = merged_df['DC1_Remain_StockQty'] + merged_df['DC2_Remain_StockQty'] + merged_df['DC4_Remain_StockQty']\n",
    "    merged_df['Remain_StockValue_AllDC'] = merged_df['DC1_Remain_StockValue'] + merged_df['DC2_Remain_StockValue'] + merged_df['DC4_Remain_StockValue']\n",
    "\n",
    "    # Calculate SO Qty\n",
    "    for dc in dc_columns:\n",
    "        merged_df[f'DC{dc}_SO_Last30D'] = round(merged_df['SO_Qty_last30D'] * merged_df[f'%Ratio_AvgSalesQty90D_DC{dc}'])\n",
    "        merged_df[f'DC{dc}_SO_Last7D'] = round(merged_df['SO_Qty_last7D'] * merged_df[f'%Ratio_AvgSalesQty90D_DC{dc}'])\n",
    "\n",
    "        merged_df[f'DC{dc}_AvgSaleQty30D'] = merged_df[f'DC{dc}_SO_Last30D'] / 30\n",
    "        merged_df[f'DC{dc}_AvgSaleQty7D'] = merged_df[f'DC{dc}_SO_Last7D'] / 7\n",
    "\n",
    "    merged_df['Total_AvgSaleQty30D'] = merged_df[[f'DC{dc}_AvgSaleQty30D' for dc in dc_columns]].sum(axis=1)\n",
    "    merged_df['Total_AvgSaleQty7D'] = merged_df[[f'DC{dc}_AvgSaleQty7D' for dc in dc_columns]].sum(axis=1)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Simplify DOH calculation\n",
    "def calculate_DOH(stock_qty, avg_qty):\n",
    "    return np.where(\n",
    "        (stock_qty != 0) & (avg_qty == 0), 365,\n",
    "        np.where((stock_qty == 0) & (avg_qty == 0), 0, stock_qty / avg_qty)\n",
    "    )\n",
    "\n",
    "# Simplified DOH calculations for various stock locations\n",
    "def apply_doh_calculations(merged_df):\n",
    "    dc_list = ['DC1', 'DC2', 'DC4']\n",
    "    max_doh_value = 1825\n",
    "    current_date = pd.to_datetime(datetime.now().date())\n",
    "\n",
    "    # Calculate DC DOH and DOH after PO for each DC\n",
    "    for dc in dc_list:\n",
    "        merged_df[f'Current_{dc}_DOH'] = calculate_DOH(\n",
    "            merged_df[f'{dc}_Remain_StockQty'],\n",
    "            merged_df[f'{dc}_AvgSaleQty90D']\n",
    "        )\n",
    "        merged_df[f'{dc}_DOH(Stock+PO)'] = calculate_DOH(\n",
    "            merged_df[f'{dc}_Remain_StockQty'] + merged_df[f'Total-PO_qty_to_{dc}'],\n",
    "            merged_df[f'{dc}_AvgSaleQty90D']\n",
    "        )\n",
    "\n",
    "    # Special columns for all DCs\n",
    "    merged_df['Current_DOH_All_DC'] = calculate_DOH(\n",
    "        merged_df['Remain_StockQty_AllDC'],\n",
    "        merged_df['Total_AvgSaleQty90D']\n",
    "    )\n",
    "    merged_df['Total-PO_qty_to_DC'] = merged_df[[f'Total-PO_qty_to_{dc}' for dc in dc_list]].sum(axis=1)\n",
    "\n",
    "    merged_df['Current_DOH(Stock+PO)_All_DC'] = calculate_DOH(\n",
    "        merged_df['Remain_StockQty_AllDC'] + merged_df['Total-PO_qty_to_DC'],\n",
    "        merged_df['Total_AvgSaleQty90D']\n",
    "    )\n",
    "\n",
    "    # Ensure all delivery date columns are datetime\n",
    "    dc_date_columns = {\n",
    "        'DC1': ['Min_del_date_to_DC1'],\n",
    "        'DC2': ['Min_del_date_to_DC2'],\n",
    "        'DC4': ['Min_del_date_to_DC4']\n",
    "    }\n",
    "\n",
    "    for dc, cols in dc_date_columns.items():\n",
    "        for col in cols:\n",
    "            data_cols = merged_df[col].replace(['', '0', 0, 'null', 'None', '#N/A'], pd.NaT)\n",
    "            data_cols = pd.to_datetime(data_cols, errors='coerce')\n",
    "            data_cols = data_cols.where(~(data_cols.dt.date == pd.Timestamp('1970-01-01').date()), pd.NaT)\n",
    "\n",
    "            merged_df[col] = data_cols       \n",
    "\n",
    "        # Calculate Min deld ate only if at least one column has a non-null value\n",
    "        merged_df[f'Min_delivery_date_to_{dc}'] = merged_df[cols].min(axis=1)\n",
    "\n",
    "    merged_df['Min_delivery_date_to_DC'] = merged_df[[f'Min_delivery_date_to_{dc}' for dc in dc_list]].min(axis=1)\n",
    "\n",
    "    # Cap DOH values\n",
    "    doh_columns = [\n",
    "        'Current_DOH_All_DC', 'Current_DOH(Stock+PO)_All_DC'\n",
    "    ] + [f'Current_{dc}_DOH' for dc in dc_list] + [f'{dc}_DOH(Stock+PO)' for dc in dc_list]\n",
    "\n",
    "    for col in doh_columns:\n",
    "        merged_df[col] = np.where(merged_df[col] > max_doh_value, np.inf, merged_df[col])\n",
    "\n",
    "    # Initialize cover date columns\n",
    "    cover_date_cols = [\n",
    "        'Total_Store_cover_to_date', 'Stock_All_DC_Cover_to_date', 'Stock+PO_All_DC_Cover_to_date'\n",
    "    ] + [f'{prefix}_{dc}_cover_to_date' for dc in dc_list for prefix in ['Store', 'Stock', 'Stock+PO']]\n",
    "\n",
    "    for col in cover_date_cols:\n",
    "        merged_df[col] = pd.NaT\n",
    "\n",
    "    return merged_df, current_date, max_doh_value\n",
    "\n",
    "\n",
    "# Cover date calculation function\n",
    "def apply_cover_date_calculations(merged_df, current_date, max_doh_value):\n",
    "    cover_date_map = {\n",
    "        'Total_Store_cover_to_date': 'Total_DOHStore',\n",
    "        'Stock_All_DC_Cover_to_date': 'Current_DOH_All_DC',\n",
    "        'Stock+PO_All_DC_Cover_to_date': 'Current_DOH(Stock+PO)_All_DC',\n",
    "        'Store_DC1_cover_to_date': 'DC1_DOHStore',\n",
    "        'Stock_DC1_cover_to_date': 'Current_DC1_DOH',\n",
    "        'Stock+PO_DC1_cover_to_date': 'DC1_DOH(Stock+PO)',\n",
    "        'Store_DC2_cover_to_date': 'DC2_DOHStore',\n",
    "        'Stock_DC2_cover_to_date': 'Current_DC2_DOH',\n",
    "        'Stock+PO_DC2_cover_to_date': 'DC2_DOH(Stock+PO)',\n",
    "        'Store_DC4_cover_to_date': 'DC4_DOHStore',\n",
    "        'Stock_DC4_cover_to_date': 'Current_DC4_DOH',\n",
    "        'Stock+PO_DC4_cover_to_date': 'DC4_DOH(Stock+PO)'\n",
    "    }\n",
    "\n",
    "    def valid_doh_to_date(doh_value):\n",
    "        return pd.notnull(doh_value) and doh_value > 0 and not np.isinf(doh_value) and doh_value <= max_doh_value\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        for target_col, source_col in cover_date_map.items():\n",
    "            doh_value = row[source_col]\n",
    "            if valid_doh_to_date(doh_value):\n",
    "                merged_df.at[index, target_col] = current_date + pd.to_timedelta(doh_value, unit='d')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    dataframes = load_data(file_paths)\n",
    "    access_df2 = load_access_data()\n",
    "    access_df2 = convert_cj_item_to_string(dataframes, access_df2)\n",
    "    \n",
    "    merged_df = merge_dataframes(dataframes, access_df2)\n",
    "\n",
    "    # Merge with master Owner and lead time to get owner name and lead time\n",
    "    merged_df = merged_df.merge(dataframes['Master_Owner&LT'],on=['SHM_Item'],how='left', suffixes = ('', '_from-LT'))\n",
    "    merged_df = merged_df.fillna({'OwnerSCM': 'No data', 'LeadTime': 'No data'})\n",
    "\n",
    "    merged_df = fill_na_with_zero(merged_df)\n",
    "    merged_df = calculate_totals(merged_df)\n",
    "    \n",
    "    merged_df, current_date, max_doh_value = apply_doh_calculations(merged_df)\n",
    "    merged_df = apply_cover_date_calculations(merged_df, current_date, max_doh_value)\n",
    "\n",
    "   # Execute SQL query\n",
    "    query = \"\"\"\n",
    "    SELECT Division_SHM,\n",
    "        OwnerSCM,\n",
    "        [Supplier Name],\n",
    "        SHM_Item,\n",
    "        CJ_Item,\n",
    "        Name,\n",
    "        Category,\n",
    "        Brand,\n",
    "        LeadTime,\n",
    "        Status,\n",
    "        [Group],\n",
    "        Unit,\n",
    "        PC_Cartons,\n",
    "        First_SO_Date,\n",
    "        NPD_Status,\n",
    "        Total_ScmAssort,\n",
    "        Total_OOSAssort,\n",
    "        Total_CountOKROOS,\n",
    "        Total_PercOOS,\n",
    "        Total_StoreStockQty,\n",
    "        Total_DOHStore,\n",
    "        Total_Store_cover_to_date,\n",
    "        Total_AvgSaleQty90D,\n",
    "        Total_AvgSaleQty30D,\n",
    "        Total_AvgSaleQty7D,\n",
    "        SO_Qty_last7D,\n",
    "        Remain_StockQty_AllDC,\n",
    "        Current_DOH_All_DC,\n",
    "        Stock_All_DC_Cover_to_date,\n",
    "        [Total-PO_qty_to_DC],\n",
    "        Min_delivery_date_to_DC,\n",
    "        [Current_DOH(Stock+PO)_All_DC],\n",
    "        [Stock+PO_All_DC_Cover_to_date],\n",
    "        DC1_ScmAssort,\n",
    "        DC1_OOSAssort,\n",
    "        DC1_CountOKROOS,\n",
    "        DC1_PercOOS,\n",
    "        DC1_StoreStockQty,\n",
    "        DC1_DOHStore,\n",
    "        Store_DC1_cover_to_date,\n",
    "        DC1_AvgSaleQty90D,\n",
    "        DC1_AvgSaleQty30D,\n",
    "        DC1_AvgSaleQty7D,\n",
    "        [%Ratio_AvgSalesQty90D_DC1],\n",
    "        DC1_Remain_StockQty,\n",
    "        Current_DC1_DOH,\n",
    "        Stock_DC1_cover_to_date,\n",
    "        [Total-PO_qty_to_DC1],\n",
    "        Min_delivery_date_to_DC1,\n",
    "        [DC1_DOH(Stock+PO)],\n",
    "        [Stock+PO_DC1_cover_to_date],\n",
    "        DC2_ScmAssort,\n",
    "        DC2_OOSAssort,\n",
    "        DC2_CountOKROOS,\n",
    "        DC2_PercOOS,\n",
    "        DC2_StoreStockQty,\n",
    "        DC2_DOHStore,\n",
    "        Store_DC2_cover_to_date,\n",
    "        DC2_AvgSaleQty90D,\n",
    "        DC2_AvgSaleQty30D,\n",
    "        DC2_AvgSaleQty7D,\n",
    "        [%Ratio_AvgSalesQty90D_DC2],\n",
    "        DC2_Remain_StockQty,\n",
    "        Current_DC2_DOH,\n",
    "        Stock_DC2_cover_to_date,\n",
    "        [Total-PO_qty_to_DC2],\n",
    "        Min_delivery_date_to_DC2,\n",
    "        [DC2_DOH(Stock+PO)],\n",
    "        [Stock+PO_DC2_cover_to_date],\n",
    "        DC4_ScmAssort,\n",
    "        DC4_OOSAssort,\n",
    "        DC4_CountOKROOS,\n",
    "        DC4_PercOOS,\n",
    "        DC4_StoreStockQty,\n",
    "        DC4_DOHStore,\n",
    "        Store_DC4_cover_to_date,\n",
    "        DC4_AvgSaleQty90D,\n",
    "        DC4_AvgSaleQty30D,\n",
    "        DC4_AvgSaleQty7D,\n",
    "        [%Ratio_AvgSalesQty90D_DC4],\n",
    "        DC4_Remain_StockQty,\n",
    "        Current_DC4_DOH,\n",
    "        Stock_DC4_cover_to_date,\n",
    "        [Total-PO_qty_to_DC4],\n",
    "        Min_delivery_date_to_DC4,\n",
    "        [DC4_DOH(Stock+PO)],\n",
    "        [Stock+PO_DC4_cover_to_date]\n",
    "    FROM merged_df\n",
    "    WHERE ([Group] != 'Discontinuous' or [Group] IS NULL)\n",
    "    ORDER BY CJ_Item ASC\n",
    "    \"\"\"\n",
    "\n",
    "    result_df = ps.sqldf(query, locals())\n",
    "    \n",
    "\n",
    "    # Rename All column for store to display a data this column is freezed\n",
    "    columns_to_rename = {\n",
    "        'Status': 'CJ_Status',\n",
    "        'Group': 'SHM_Status',\n",
    "        'Unit': 'Unit_of_Purchase',\n",
    "        'LeadTime':'LeadTime(Days)',\n",
    "        'Total_OOSAssort':'Total_ActiveAssort',\n",
    "        'Total_CountOKROOS':'Total_StoreOOS',\n",
    "        'Total_PercOOS':'Total_%StoreOOS'\n",
    "        }\n",
    "    \n",
    "    # Rename the columns\n",
    "    result_df.rename(columns=columns_to_rename, inplace=True)\n",
    "    renamed_columns = [columns_to_rename[old_name] for old_name in columns_to_rename if old_name in result_df.columns]\n",
    "    print(f\"The following columns have been renamed: {columns_to_rename}\")\n",
    "\n",
    "\n",
    "    # Filter to show the data only all numeric column > 0\n",
    "    numeric_cols = result_df.select_dtypes(include=[np.number]).columns\n",
    "    result_df = result_df[(result_df[numeric_cols] != 0).any(axis=1)]\n",
    "\n",
    "    # Format the date columns to display in Excel\n",
    "    date_columns = [\n",
    "        'First_SO_Date',\n",
    "        'Min_delivery_date_to_DC',\n",
    "        'Min_delivery_date_to_DC1',\n",
    "        'Min_delivery_date_to_DC2',\n",
    "        'Min_delivery_date_to_DC4',\n",
    "        'Total_Store_cover_to_date',\n",
    "        'Store_DC1_cover_to_date',\n",
    "        'Store_DC2_cover_to_date',\n",
    "        'Store_DC4_cover_to_date',\n",
    "        'Stock_All_DC_Cover_to_date',\n",
    "        'Stock_DC1_cover_to_date',\n",
    "        'Stock_DC2_cover_to_date',\n",
    "        'Stock_DC4_cover_to_date',\n",
    "        'Stock+PO_All_DC_Cover_to_date',\n",
    "        'Stock+PO_DC1_cover_to_date',\n",
    "        'Stock+PO_DC2_cover_to_date',\n",
    "        'Stock+PO_DC4_cover_to_date'\n",
    "    ]\n",
    "\n",
    "    for col in date_columns:\n",
    "        if col in result_df.columns:\n",
    "            result_df[col] = pd.to_datetime(result_df[col], errors='coerce')  # Ensure it's in datetime format\n",
    "    \n",
    "\n",
    "    # Drop SHM_Item = No shm item in access and Save deduplicated_df to excel\n",
    "    #result_df.to_excel(r'D:\\Data for Stock Report\\Data_for_OOS_Log\\11.Nov_24-11-2025.xlsx', index=False)\n",
    "\n",
    "    # Handle duplicates 'CJ_Item' by using groupby\n",
    "    def replace_cj_duplicate(group):\n",
    "        special_products = ['20000408','20009203','20014191','20023778','20023779','20028264','20039186'] # deal with Sup = \"ซัน ซัน\"\n",
    "        is_special_product = group['CJ_Item'].iloc[0] in special_products\n",
    "        dc4_columns = [col for col in numeric_cols if 'DC4' in col] # get all column name contains 'DC4' and numeric type\n",
    "        \n",
    "        # if special product and only one row then return the group\n",
    "        if is_special_product and len(group) == 1:\n",
    "            return group\n",
    "        \n",
    "        if is_special_product: # if special product replace value with 0 for first rows contains 'DC4' in column name\n",
    "            for col in numeric_cols:\n",
    "                if col in dc4_columns or col == 'PC_Cartons':\n",
    "                    continue\n",
    "                if group[col].nunique() == 1:\n",
    "                    group.iloc[1:, group.columns.get_loc(col)] = 0\n",
    "\n",
    "            for col in dc4_columns:\n",
    "                if group[col].nunique() == 1:\n",
    "                    group.iloc[0, group.columns.get_loc(col)] = 0\n",
    "\n",
    "        # Filter these 2 columns if both columns are 0 then drop the row, keep the first row\n",
    "        #group = group[(group[['Remain_StockQty_AllDC', 'Total-PO_qty_to_DC']].ne(0).any(axis=1)) | (group.index == group.index.min())]\n",
    "\n",
    "        return group\n",
    "    \n",
    "    # Apply this function to each group of 'CJ_Item'\n",
    "    result_df = result_df.groupby('CJ_Item', group_keys=False).apply(replace_cj_duplicate).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    # Rename column for converting QTY to BOX QTY\n",
    "    column_rename_mapping = {\n",
    "        'Total_AvgSaleQty90D': 'Total_AvgSaleCTN_Last90D',\n",
    "        'Total_AvgSaleQty30D': 'Total_AvgSaleCTN_Last30D',\n",
    "        'Total_AvgSaleQty7D': 'Total_AvgSaleCTN_Last7D',\n",
    "        'DC1_AvgSaleQty90D': 'DC1_AvgSaleCTN_Last90Days',\n",
    "        'DC1_AvgSaleQty30D': 'DC1_AvgSaleCTN_Last30Days',\n",
    "        'DC1_AvgSaleQty7D': 'DC1_AvgSaleCTN_Last7Days',\n",
    "        'DC2_AvgSaleQty90D': 'DC2_AvgSaleCTN_Last90Days',\n",
    "        'DC2_AvgSaleQty30D': 'DC2_AvgSaleCTN_Last30Days',\n",
    "        'DC2_AvgSaleQty7D': 'DC2_AvgSaleCTN_Last7Days',\n",
    "        'DC4_AvgSaleQty90D': 'DC4_AvgSaleCTN_Last90Days',\n",
    "        'DC4_AvgSaleQty30D': 'DC4_AvgSaleCTN_Last30Days',\n",
    "        'DC4_AvgSaleQty7D': 'DC4_AvgSaleCTN_Last7Days',\n",
    "        'Total_StoreStockQty': 'Total_StoreStockCTN',\n",
    "        'DC1_StoreStockQty': 'DC1_StoreStockCTN',\n",
    "        'DC2_StoreStockQty': 'DC2_StoreStockCTN',\n",
    "        'DC4_StoreStockQty': 'DC4_StoreStockCTN',\n",
    "        'SO_Qty_last7D': 'SO_CTN_last7D',\n",
    "        'Remain_StockQty_AllDC': 'Remain_CTN_AllDC',\n",
    "        'DC1_Remain_StockQty': 'DC1_Remain_CTN',\n",
    "        'DC2_Remain_StockQty': 'DC2_Remain_CTN',\n",
    "        'DC4_Remain_StockQty': 'DC4_Remain_CTN',\n",
    "        'Total-PO_qty_to_DC': 'Total-CTN_to_DC',\n",
    "        'Total-PO_qty_to_DC1': 'Total-CTN_to_DC1',\n",
    "        'Total-PO_qty_to_DC2': 'Total-CTN_to_DC2',\n",
    "        'Total-PO_qty_to_DC4': 'Total-CTN_to_DC4'\n",
    "    }\n",
    "\n",
    "    # Process the rename\n",
    "    modified_df = result_df.rename(columns=column_rename_mapping)\n",
    "\n",
    "    # List of new columns that were just renamed\n",
    "    renamed_columns = list(column_rename_mapping.values())\n",
    "    \n",
    "    # Loop through renamed column to perform division by PC_Cartons\n",
    "    for col in renamed_columns:\n",
    "        modified_df[col] = np.where(\n",
    "            modified_df['PC_Cartons'] == 0,0,\n",
    "            modified_df[col] / modified_df['PC_Cartons']\n",
    "        )\n",
    "\n",
    "    # Define current date and save path \n",
    "    current_date = datetime.now().strftime('%d-%m-%Y')\n",
    "    save_directory = r'D:\\Data for Stock Report\\Completed Daily Stock Report'\n",
    "    file_name = f\"Sahamit Daily Stock Report_{current_date}.xlsx\"\n",
    "    save_path = os.path.join(save_directory, file_name)\n",
    "\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    # Save the merged DataFrame\n",
    "    try: \n",
    "        with pd.ExcelWriter(save_path, mode='w') as writer:\n",
    "            result_df.to_excel(writer, sheet_name='Data by Qty', index=False)\n",
    "            modified_df.to_excel(writer, sheet_name='Data by Cartons',index=False)\n",
    "\n",
    "            # For checking Raw data\n",
    "            #merged_df.to_excel(writer, sheet_name='Raw Data', index=False)\n",
    "\n",
    "            # Load All sheet from Raw_PO_pending and save them together\n",
    "            existing_file_path = r'D:\\Data for Stock Report\\Raw_PO_pending.xlsx'\n",
    "            if os.path.exists(existing_file_path):\n",
    "                workbook = load_workbook(existing_file_path)\n",
    "                for sheet_name in workbook.sheetnames:\n",
    "                    existing_df = pd.read_excel(existing_file_path,sheet_name=sheet_name)\n",
    "                    existing_df.to_excel(writer,sheet_name=sheet_name,index=False)\n",
    "            else:\n",
    "                print(f\"File not found: {existing_file_path}\")\n",
    "\n",
    "        print(f\"Completed daily stock report has been saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
